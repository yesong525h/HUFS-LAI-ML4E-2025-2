# 4th Assignment: Data Collection and Analysis

## 과제 개요

이번 과제는 Assignment 3에서 제안한 프로젝트의 **데이터 수집 및 탐색적 분석(EDA: Exploratory Data Analysis)** 단계입니다.

좋은 모델은 좋은 데이터에서 시작됩니다. 이 단계에서는 본인의 프로젝트에 필요한 데이터를 실제로 수집하고, 데이터를 이해하기 위해 탐색하고 분석합니다.

### 전체 프로젝트 구성

| Assignment | 주제 | 설명 |
|-----------|------|------|
| Assignment 3 | Project Proposal | 프로젝트 주제 설정 및 제안서 작성 |
| **Assignment 4** | **Data Collection and Analysis** | **데이터 수집 및 분석** |
| Assignment 5 | Model Training and Evaluation | 모델 학습 및 평가 |
| Assignment 6 | Real Usage and Final Report | 실제 사용 및 최종 보고서 |

### Assignment 4의 목표

- 프로젝트에 필요한 데이터를 실제로 수집합니다
- 수집한 데이터를 탐색하고 분석합니다
- 데이터의 특성, 분포, 결측값 등을 파악합니다
- 데이터 전처리 계획을 수립합니다

**이 단계는 모델 학습만큼 중요합니다.** 데이터를 잘 이해하지 못하면 좋은 모델을 만들 수 없습니다.

### TL;DR

| 항목 | 설명 |
|------|------|
| **데이터 수집** | 실제로 사용할 데이터를 수집 (수동/크롤링/API/공개 데이터셋 등) |
| **데이터 분석** | 기본 통계, 분포, 시각화로 데이터 특성 파악 |
| **보고 수준** | 1-2시간 정도의 EDA 분석으로 충분 (완벽할 필요 없음) |
| **기술 요구** | 간단한 Python + Pandas/Matplotlib으로 충분 |

---

## 제출 방법

`submissions/{학번}/assignment4/` 디렉토리에 분석 코드와 데이터를 제출하세요.

### 제출 구조

```
submissions/{학번}/assignment4/
├── data-analysis.ipynb        # (필수) Jupyter Notebook 또는 Colab 코드
├── data/                       # (필수) 데이터 파일
│   ├── data.csv (또는 .json)
│   └── (필요시) 추가 데이터 파일들
└── README.md  # (필수) 분석 결과 요약
```

### 제출 단계

1. [https://github.com/HUFS-LAI-Seungtaek/HUFS-LAI-ML4E-2025-2](https://github.com/HUFS-LAI-Seungtaek/HUFS-LAI-ML4E-2025-2) 접속
2. 본인의 Fork repository에서 작업
3. 분석 코드를 `data-analysis.ipynb`로 작성
   - Google Colab 사용 가능 → `.ipynb` 파일로 다운로드 후 commit
4. 수집한 데이터를 `data/` 디렉토리에 저장
   - **데이터 형식**: CSV 혹은 JSON (간단하고 GitHub에서 미리보기 가능. 그 외의 포맷에 대해서는 문의.)
   - **파일 크기**: 100MB 이하 권장 (100MB 초과 시 구글 드라이브 링크로 제출)
5. 제출 후 PR 생성
    - Title: `4th Assignment by {학번} ({영어 이름})`
        - 예시: `4th Assignment by 2025122 (Seungtaek Choi)`
    - Description:
        ```markdown
        - `submissions/{학번}/assignment4/` 디렉토리 제출합니다.
        - 수집한 데이터: {데이터 출처 및 개수}
        - 데이터 파일: {파일명} ({크기})
        ```

---

## 평가 기준

### Assignment 4 평가 기준

이 과제는 **Pass / Non-Pass**로만 평가됩니다.

**Pass 기준**:
1. **제출 구조 완성**: PR이 정상적으로 생성되고 merge 가능한 상태
   - `data-analysis.ipynb` 파일 필수 제출
   - `data/` 디렉토리에 데이터 파일 필수 포함

2. **데이터 및 분석 적절성**: 다음 조건을 만족
   - 수집한 데이터가 프로젝트와 관련성 있음
   - 코드에 기본 통계, 시각화 등의 분석 포함
   - 데이터의 특성을 이해할 수 있을 정도의 분석량

**Non-Pass 사유**:
- PR 형식이 맞지 않거나 필수 파일 누락
- `.ipynb` 파일 또는 데이터 파일 미제출
- 수집한 데이터가 프로젝트와 무관함
- 분석 내용 거의 없음 (코드만 있고 실행/분석 없음)

---

## Jupyter Notebook 작성 가이드

`data-analysis.ipynb` 파일에 분석 코드를 작성하세요. **Markdown 파일 대신 Jupyter Notebook을 주요 제출물으로 생각하세요.**

### 최소한 포함되어야 할 내용

Notebook에 다음 요소들이 포함되면 좋습니다:

1. **데이터 로드 및 기본 정보**
   - 데이터 읽기 (Pandas)
   - 데이터 크기, 칼럼, 타입 확인

2. **기본 통계**
   - `df.describe()`, `df.info()` 등
   - 결측값 확인

3. **시각화** (최소 1-2개)
   - 분포: 히스토그램, 박스플롯
   - 관계: 산점도, 상관계수 행렬 (heatmap)

4. **분석 및 발견 사항**
   - Markdown cell에 주요 발견 정리
   - 데이터 품질 문제 기록

---

## 프로젝트별 분석 아이디어

각 프로젝트 타입에 따라 어떤 분석을 할 수 있을지 몇 가지 예시를 보여줍니다. **자신의 프로젝트에 맞게 선택해서 진행하세요.**

### 아이디어 1: arXiv 논문 자동 추천 봇

**가능한 분석들**:

**데이터 수집**:
- arXiv API로 최근 3-6개월 NLP 논문 300-500편 수집
- CSV: `title, abstract, authors, publish_date, relevance`

**기본 분석**:
- Abstract 길이 분포
- label (relevance) 분포
- relevance=1 인 경우에 가장 많이 등장한 단어들의 word cloud

---

### 아이디어 2: 학과 공지 자동 분류 봇

**가능한 분석들**:

**데이터 수집**:
- 학과 홈페이지에서 300-500개 공지사항 크롤링
- CSV: `title, content, date, manual_category` (수동으로 카테고리 라벨링)

**기본 분석**:
- 월별/계절별 공지 수
- 카테고리별 분포
- 텍스트 길이 (제목, 본문)
- 특정 키워드 빈도 ("긴급", "수강신청" 등)

---

### 아이디어 3: 개인화 번역 도구

**가능한 분석들**:

**데이터 수집**:
- 본인의 과거 번역 데이터 100-200개 수집
- CSV: `source_text, machine_translation, my_translation, needs_correction`

**기본 분석**:
- 문장 길이 (단어 수)
- 원문 길이 vs 수정 필요 여부
- 수정 유형 (문체, 용어, 어순 등)
- 특정 단어나 구문에서의 오류 패턴


**발견할 수 있는 것들**:
- 긴 문장일수록 오류율 높음
- 특정 용어에서 반복되는 오류
- 문체나 어순에서의 선호도

---

### 아이디어 4: 일반적인 데이터셋 분석

**다른 유형의 프로젝트라면**:

**기본 템플릿**:
1. 데이터 크기, 칼럼 정보
2. 결측값, 데이터 타입 확인
3. 기본 통계 (`describe()`)
4. 주요 칼럼의 분포 시각화
5. 특성 간 상관관계 (필요시)
6. 데이터 품질 문제 기록

**항상 확인할 것들**:
- 불균형 데이터 (한 클래스가 대부분)
- 이상치 (매우 크거나 작은 값)
- 결측값 (어느 칼럼에 많은가)
- 중복된 샘플

---

## 추가 리소스

---

## FAQ

**Q: 공개 데이터셋을 사용해도 되나요?**
A: 네, 괜찮습니다. 다만 본인의 프로젝트에 맞게 가공하거나 필터링해야 합니다. 예를 들어, Kaggle 데이터셋을 그대로 사용하기보다는 프로젝트의 필요한 부분만 추출하는 것을 권장합니다.

**Q: 데이터가 충분하지 않으면 어떻게 하나요?**
A: Assignment 4는 "분석"에 초점이 있습니다. 데이터가 50개라도 좋으니 충분히 분석해보세요. 부족한 부분은 Assignment 5나 Assignment 6에서 데이터를 추가 수집할 수 있습니다.

**Q: 이미지 데이터나 오디오는 어떻게 분석하나요?**
A: 프로젝트의 특성에 맞게 분석하세요. 이미지는 크기, 색상 분포 등을, 오디오는 길이, 주파수 등을 분석할 수 있습니다.

**Q: 얼마나 자세히 분석해야 하나요?**
A: 1-2시간 정도의 분석으로 충분합니다. 완벽하지 않아도 괜찮으니 데이터를 충분히 이해하는 것이 목표입니다.

**Q: 데이터 수집에 실패하면 어떻게 하나요?**
A: 크롤링 차단, API 문제 등의 이유로 수집 실패 시, 다른 방법을 시도하거나 유사한 공개 데이터셋을 사용해도 됩니다. 교수와 상담하세요.

**Q: 데이터를 어디에 저장하나요?**
A: 작은 데이터셋(< 100MB)은 repo에 commit해도 좋습니다. 큰 데이터는:
- Google Drive 업로드 후 링크 제공
- 공개 데이터셋이면 출처 링크만 제공

---

## 제출 마감

- **마감일**: 2025-11-26 (2 weeks)
- **제출 방법**: GitHub Pull Request
- **지각 제출**: 마감일 이후 제출 시 Non-Pass 처리될 수 있음

---

마지막으로, 데이터 분석은 "모델 학습의 밑바탕"입니다. 지금 데이터를 잘 이해하면 Assignment 5에서 훨씬 더 나은 모델을 만들 수 있습니다.
