# 🧠 MNIST 분류 실험 결과

## 📊 기본 모델 성능 (10회 반복)
- **최종 테스트 정확도:** 97.68% ± 0.18%  
- **평균 훈련 시간:** 0분 46.71초 ± 1.10초  

---

## ⚙️ 실험 1: 하이퍼파라미터 튜닝

### 🔧 변경 사항
- **학습률 (Learning Rate):**  
  1e-3 → 5e-4 → 1e-4 → 5e-3 → 1e-2 (5회 반복)
- **배치 크기 (Batch Size):**  
  128 → 118 → 138 → 148 → 200 (학습률 1e-3 유지, 5회 반복)
- **에포크 수 (Epochs):**  
  3 → 2 → 4 → 5 (배치 크기 148, 학습률 1e-3 유지, 5회 반복)
- **은닉층 개수 (Hidden Units):**  
  100 → 500 → 1000 → 600 → 589 → 550  
  (에포크 4, 배치 148, 학습률 1e-3 유지, 5회 반복)

### 📈 결과
- **정확도 변화:** 97.68% ± 0.18% → 97.66% ± 0.27%  
- **훈련 시간:** 0분 46.71초 ± 1.10초 → 1분 7.04초 ± 2.36초  

**분석:**  
- 정확도는 소폭 감소, 표준편차 증가로 **변동성 증가**  
- 학습률(1e-3)을 유지하되 **배치 크기, 은닉층, 에포크 수의 균형 조정**이 중요  
- 에포크 수 조정은 정확도뿐 아니라 **훈련 시간에도 큰 영향**  
- 조정 후 개선이 없다면 **기본 모델 유지가 더 효율적**

---

## 🧩 실험 2: 모델 구조 개선

### 🔧 변경 사항
- **은닉층 구성:** 기존 2층 → 3층 (589 → 256 → 128)  
- **활성 함수:**  
  - 1층·3층: LeakyReLU  
  - 2층: Tanh  
- **정규화:** 모든 층에 BatchNormalization(1d) 적용  
- **Dropout:** 모든 층에 p=0.3 적용  

### 📈 결과
- **정확도:** 97.68% ± 0.18% → 97.77% ± 0.31% (유의미한 향상)  
- **훈련 시간:** 1분 7.04초 ± 2.36초 → 1분 2.64초 ± 1.29초 (감소)  

**분석:**  
- 단순히 층을 늘리는 것보다 **LeakyReLU + Tanh 조합**이 성능 향상에 핵심  
- **Dropout과 BatchNorm**은 정확도보다 **학습 안정화**에 기여  

---

## 🧭 결론 및 인사이트

### ✅ 가장 효과적인 개선 조합
- **하이퍼파라미터:** lr=1e-3, batch=148, epoch=4  
- **모델 구조:** 3층 네트워크 (589 → 256 → 128)  
- **활성 함수:** LeakyReLU + Tanh  
- **정규화/규제:** BatchNorm1d + Dropout(0.3)  

➡️ **정확도 향상, 학습 안정성 증가, 과적합 방지, 효율적 연산 유지**

---

### 🔍 주요 관찰
- 모델의 **깊이는 표현력 향상**에 기여하지만, 지나치면 **일반화 성능 저하**  
- **비선형 함수의 병용**은 gradient 소실 방지 및 표현 다양성 확보에 효과적  
- **BatchNorm + Dropout 조합**은 분산 안정화에 핵심적 역할  
- **적절한 정규화·활성화 설계**는 연산량 증가를 상쇄하는 효율성 제공  

---

### 🚀 추가 개선 아이디어
- **NAdam 최적화기 적용**  
- **가중치 초기화 (Weight Initialization)** 전략 다양화  
- **가변적 활성화 함수 (Flexible Activation)** 도입  
- **잔차 연결 (Residual Connection)** 적용  

💡 차후 **정확도 향상 및 훈련 시간 단축**을 기대할 수 있음.