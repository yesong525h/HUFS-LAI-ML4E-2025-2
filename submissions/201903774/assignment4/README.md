# 4th Assignment: Data Collection and Analysis  
201903774 언어인지과학과 한형준

Assignment 3에서 제안한 **강의 슬라이드 요약/정리 도구(BonCahier AI)** 프로젝트를 위해 실제 텍스트 요약 데이터셋을 수집하고, 탐색적 데이터 분석(EDA: Exploratory Data Analysis)을 수행한 결과를 정리한 문서이다. :contentReference[oaicite:0]{index=0}  

이번 과제(Assignment 4)의 목표는 다음과 같다. :contentReference[oaicite:1]{index=1}  

- 프로젝트에 사용할 **실제 데이터를 수집**한다.
- 수집한 데이터를 **기본 통계·분포·시각화**를 통해 탐색한다.
- 데이터의 품질 및 특성을 바탕으로 **전처리 및 모델링 방향을 계획**한다.

---

## 1. 데이터 수집 요약

### 1.1 한국어 요약 데이터 – 네이버 뉴스

- **출처**: HuggingFace Datasets – `daekeun-ml/naver-news-summarization-ko`
- **형식**: 뉴스 기사 본문(`document`)과 한두 문장 요약(`summary`)으로 구성된 요약 데이터셋
- **사용 split**: `train`
- **샘플 수**: 22,194개
- **주요 컬럼**
  - `date`: 기사 작성 시각  
  - `category`: 기사 카테고리 (예: `economy`, `IT과학` 등)  
  - `press`: 언론사  
  - `title`: 기사 제목  
  - `document`: 기사 전체 본문  
  - `link`: 네이버 뉴스 URL  
  - `summary`: 사람이 작성한 요약문  

분석에 사용한 raw 데이터는 `data/naver_news_summarization_ko.csv` 파일로 저장하였다.

### 1.2 영어 요약 데이터 – BBC News Summary

- **출처**: HuggingFace Datasets – `gopalkalpande/bbc-news-summary`
- **형식**: BBC 뉴스 기사(`Articles`)와 요약문(`Summaries`)으로 구성된 데이터셋
- **사용 split**: `train`
- **샘플 수**: 2,224개
- **주요 컬럼**
  - `File_path`: 상위 카테고리(예: politics, sport 등)
  - `Articles`: 기사 전체 본문
  - `Summaries`: 요약문

분석에 사용한 raw 데이터는 `data/bbc_news_summary_en.csv` 파일로 저장하였다.

### 1.3 본 프로젝트와의 연결

- 두 데이터셋 모두 **“긴 정보성 텍스트 → 짧은 요약문”** 구조를 가지며,
- 향후 BonCahier AI에서 다루게 될 **강의 슬라이드 텍스트 → 핵심 요약/슬라이드 타이틀 생성** 태스크와 구조적으로 유사하다.
- Assignment 4에서는 이 두 데이터셋을 **슬라이드 요약 태스크의 대리(corpus proxy)** 로 사용하여, 텍스트 길이, 요약 비율, 도메인 특성 등을 파악하고 이후 전처리·모델 설계에 참고하고자 했다. :contentReference[oaicite:2]{index=2}  

---

## 2. 기본 정보 및 데이터 품질

### 2.1 한국어 데이터

- 행 수: **22,194**
- 컬럼 수: **7**
- `df.info()` 결과, 모든 컬럼이 `object` 타입(문자열)이며 숫자형 컬럼은 없다.
- `isna().sum()` 결과, **결측값 0개**로 데이터 품질은 양호하다.
- `describe(include="all")` 기준:
  - `category`는 2개의 값만 존재하며, 그 중 `economy`가 약 17,000개로 가장 많아 **경제 뉴스 비중이 매우 높음**.
  - `title`, `document`, `summary`에는 중복이 일부 존재하지만 전체 규모 대비 적은 편이다.

### 2.2 영어 데이터

- 행 수: **2,224**
- 컬럼 수: **3**
- 모든 컬럼이 `object` 타입이며, **결측값 없음**.
- `File_path`는 5개 카테고리(정치, 스포츠, 비즈니스, 기술, 엔터테인먼트 등)를 가지며, 가장 많은 카테고리는 `sport` (약 510개)로 나타났다.

---

## 3. 길이 기반 특성 분석

두 데이터셋 모두에 대해 다음과 같은 파생 변수를 추가했다.

- `doc_len`: 본문 길이(문자 수)
- `sum_len`: 요약 길이(문자 수)
- `compression_ratio`: `sum_len / doc_len` (요약 압축 비율)

### 3.1 한국어 뉴스 요약

- **평균 길이**
  - 본문 길이 평균: **약 998.6자**
  - 요약 길이 평균: **약 183.2자**
  - 평균 압축 비율: **약 0.426**
- **분포**
  - 본문 길이 히스토그램:
    - 대부분의 기사가 **500–2,000자** 사이에 몰려 있으며,
    - 일부 기사에서 4,000자 이상까지 길어지는 긴 꼬리(long tail)를 가진다.
  - 요약 길이 히스토그램:
    - 주로 **150–250자** 구간에 집중되어 있고,
    - 500자 이상인 요약은 상대적으로 드물다.
  - 압축 비율 히스토그램:
    - 대부분의 샘플은 0~0.5 사이에 위치하지만,
    - 일부에서 1 이상(요약이 본문보다 더 긴 경우) 혹은 매우 큰 값(3 이상)도 관찰되어 **이상치(outlier)** 가 존재함을 확인했다.
- **문서 길이 vs 요약 길이 산점도**
  - 상위 1% 이상치를 제거하고 산점도를 그려본 결과,
    - 요약 길이가 100–300자 사이에서 **상당히 고르게 분포**하고,
    - 본문 길이가 길어져도 요약 길이가 크게 증가하지 않는 경향이 있다.
  - 즉, 한국어 네이버 뉴스 요약은 **“본문 길이와 관계없이 거의 일정한 길이의 요약”** 을 생성하는 패턴이 강하다.

### 3.2 영어 BBC 뉴스 요약

- **평균 길이**
  - 본문 길이 평균: **약 2,263.9자**
  - 요약 길이 평균: **약 1,000.5자**
  - 평균 압축 비율: **약 0.442**
- **분포**
  - 본문 길이 히스토그램:
    - 대부분의 기사가 **1,000–4,000자** 구간에 분포하며,
    - 소수의 1만 자 이상 아웃라이어가 존재한다.
  - 요약 길이 히스토그램:
    - **500–1,500자** 사이에 고르게 분포하며,
    - 한국어 요약보다 상대적으로 길고 다양하다.
  - 압축 비율 히스토그램:
    - **0.4–0.5 근처를 중심으로 종 모양 분포**를 보이며,
    - 대부분의 샘플이 0.3–0.6 범위에 위치해 한국어 데이터보다 이상치가 적다.
- **문서 길이 vs 요약 길이 산점도**
  - 상위 1% 이상치를 제거한 산점도에서는,
    - 점들이 **거의 직선에 가깝게 밀집**해 있으며,
    - 본문이 길어질수록 요약도 비례하여 증가하는 뚜렷한 양의 상관관계를 확인했다.
  - 영어 BBC 뉴스 요약은 한국어 네이버 뉴스에 비해 **본문 길이에 따라 요약 길이도 함께 스케일링되는 패턴**을 보인다.

---

## 4. 카테고리(레이블) 분포

### 4.1 한국어 네이버 뉴스

- `category` 값은 주로 **`economy`** 와 **`IT과학`** 두 가지로 구성된다.
- 값 분포(대략):
  - `economy`: 약 17,000건  
  - 기타 카테고리(IT과학 등): 약 5,000건  
- 따라서 한국어 데이터는 **경제 뉴스에 크게 편향된 도메인**이라는 점을 확인했고, 이후 모델 학습 시 일반적인 강의 슬라이드와 도메인 차이가 있을 수 있음을 염두에 둘 필요가 있다.

### 4.2 영어 BBC 뉴스

- `File_path` 기준 상위 카테고리는 `sport`, `politics`, `business`, `tech`, `entertainment` 등 총 5개이다.
- 스포츠/정치 기사 비중이 상대적으로 높으며, 다양한 주제의 텍스트를 포함한다.

---

## 5. 레이블링 데이터셋(text, label) 구성

Assignment 4 설명서에서 제시한 “레이블이 포함된 데이터 수집” 요구사항을 만족하기 위해, 두 데이터셋을 **텍스트(text)와 레이블(label)** 구조로 재구성하였다. :contentReference[oaicite:3]{index=3}  

### 5.1 한국어: `korean_text_label.csv`

- **파일 경로**: `data/korean_text_label.csv`
- **구조**
  - `text`: 기사 본문(`document`)
  - `label`: 기사 카테고리(`category` – economy, IT과학 등)
- **레이블 분포**
  - `label` 값은 4장에서 본 것처럼 경제 뉴스가 다수를 차지한다.
- 이 파일은 이후 **한국어 요약/분류 모델** 학습 시, 입력 텍스트와 도메인 레이블을 함께 활용하기 위한 기반 데이터로 사용될 수 있다.

### 5.2 영어: `english_text_label.csv`

- **파일 경로**: `data/english_text_label.csv`
- **구조**
  - `text`: 기사 본문(`Articles`)
  - `label`: 상위 카테고리(`File_path` – politics, sport, business, tech, entertainment)
- **레이블 분포**
  - sport, politics 등 몇몇 카테고리가 상대적으로 많은 비중을 차지한다.
- 이 파일은 **영어 요약/분류 모델** 혹은 다국어 모델을 위한 학습 데이터로 활용 가능하다.

---

## 6. 단어 빈도 분석 (전체 / 레이블별)

Assignment 4의 “데이터 분포뿐만 아니라 가장 많이 등장하는 단어, 클래스별 자주 등장하는 단어 분석” 요구를 반영하여, 한국어·영어 데이터에 대해 **단어 빈도 기반 탐색**을 수행하였다. :contentReference[oaicite:4]{index=4}  

### 6.1 분석 방법

1. **토크나이징**
   - 한국어:
     - 정규식을 사용하여 한글/영문/숫자/공백만 남기고 나머지 기호 제거
     - 공백 기준으로 나눈 뒤, 길이 1인 토큰은 제외
   - 영어:
     - 소문자로 변환 후, 알파벳으로만 이루어진 토큰을 추출
     - 길이 1인 토큰은 제외
2. **전체 코퍼스 단어 빈도**
   - 한국어/영어 각각에 대해 `Counter`를 이용해 **상위 N개 단어**의 등장 빈도를 계산
   - 계산량을 줄이기 위해 한국어는 최대 10,000개 문서, 영어는 최대 2,000개 문서를 샘플링하여 사용
3. **레이블별 단어 빈도**
   - `korean_text_label.csv`, `english_text_label.csv`를 기반으로
     - `label` 값별로 텍스트를 모은 뒤,
     - 각 레이블 그룹 내에서 상위 N개 단어를 계산
   - 이를 통해, 카테고리별로 **어떤 단어들이 특징적으로 자주 등장하는지**를 파악하였다.

### 6.2 관찰한 패턴 (정성적 요약)

- 한국어 네이버 뉴스:
  - 전체적으로 **경제·정책·시장·기업 관련 단어들**이 높은 빈도로 등장하며,
  - `economy`와 다른 카테고리 간에 사용되는 어휘 분포가 다름을 확인할 수 있었다.
- 영어 BBC 뉴스:
  - `sport` 카테고리에서는 경기·선수·점수 등 스포츠 관련 어휘가,
  - `politics` 카테고리에서는 정부·정책·선거 등 정치 관련 어휘가 상대적으로 많이 등장하는 양상이 관찰되었다.
- 이러한 단어 분포는 이후
  - **도메인별 요약 스타일 차이 분석**,
  - 혹은 **간단한 베이스라인 분류 모델(BoW + Logistic Regression 등)** 을 만들 때 유용한 피처로 사용될 수 있다.

상세한 상위 단어 목록과 빈도 수치는 `data-analysis.ipynb`의 해당 셀 출력에 포함되어 있으며, 본 README에서는 분석 절차와 정성적 패턴을 중심으로 요약하였다.

---

## 7. 데이터 전처리 계획

Assignment 5 이후 모델 학습 단계에서 사용할 데이터 전처리 전략은 다음과 같이 계획하였다. :contentReference[oaicite:5]{index=5}  

### 7.1 길이 및 압축 비율 기반 필터링

1. **최소 길이 필터**
   - 본문이 지나치게 짧은 샘플(예: `doc_len < 100`)은 강의 슬라이드 수준의 설명 텍스트로 보기 어렵기 때문에 제거.
   - 요약이 너무 짧은 샘플(예: `sum_len < 20`)도 품질이 낮을 가능성이 있어 제거.

2. **압축 비율 필터**
   - 한국어:
     - 이상치 제거를 위해 `compression_ratio`가 **0.05 미만**(거의 요약이 없는 수준) 또는 **0.8 이상**(요약이 본문과 거의 같은 길이 혹은 더 긴 경우)인 샘플은 제외.
   - 영어:
     - 분포가 비교적 안정적이므로 `0.2 ≤ compression_ratio ≤ 0.7` 정도 범위를 기본으로 두고, 극단적인 outlier만 추가로 제거.

3. **길이 상한 설정**
   - 후속 모델 학습에서 BART/KoBART 계열 모델을 사용할 것을 고려하여,
     - 본문 입력은 최대 **약 3,000자 내외**(토큰 기준 약 1,024–1,536 토큰),
     - 요약 출력은 최대 **약 400–600자** 수준으로 클리핑(truncation)할 계획이다.

### 7.2 텍스트 정제

1. **공통 정제**
   - 공백/줄바꿈/탭 문자 정규화
   - 연속된 공백을 하나로 축소
   - HTML 엔티티, 불필요한 기호(예: `\xa0`, 특수 문자 등) 제거

2. **한국어 뉴스 특화 정제**
   - 뉴스 문장의 서두에 자주 등장하는 패턴(예: `서울=연합뉴스`, `○○기자`, `[앵커]`)을 정규식으로 제거
   - 요약문에서 반복되는 날짜, 매체 이름 등의 **비내용적 정보**를 가능한 한 줄여, 강의 개요/슬라이드 타이틀에 더 가까운 요약이 되도록 전처리

3. **영어 뉴스 특화 정제**
   - 불필요한 인용부호, 헤드라인용 대문자 스타일 정리
   - 목록형 요약(예: `- item1 - item2 ...`)의 경우, bullet 기호(`-`, `•`)를 제거하고 자연스러운 문장형으로 변환하는 전처리를 검토

### 7.3 데이터 분할 및 샘플링

- HuggingFace 데이터셋의 기본 split 구조(train/validation/test)가 존재하므로,
  - 가능하면 **기존 split을 존중**하되,
  - Assignment 4에서는 train만 사용한 만큼,
    Assignment 5에서는 train 내에서 80/10/10 비율로 재분할하여 **train/validation/test를 명시적으로 구성**하는 것도 고려한다.
- 한국어(22k)와 영어(2.2k)의 데이터 규모 차이가 크기 때문에,
  - 다국어 모델을 학습할 경우 영어 데이터를 기준으로 한국어를 다운샘플하거나,
  - 언어별 별도 모델을 학습하는 전략(예: 한국어 KoBART vs 영어 BART)을 병행 검토한다.

### 7.4 강의 슬라이드 데이터와의 연결

- 이번 Assignment 4에서는 공개 뉴스 요약 데이터만 사용했지만, Notebook에서 PPTX 파일의 슬라이드 텍스트를 추출하는 함수 틀을 구현해 두었다.
- 이후 실제 강의 자료(PPT/PDF)를 불러와,
  - 슬라이드별 텍스트 길이 분포를 측정하고,
  - 위에서 관찰한 뉴스 본문 길이 분포와 비교하여
    - “강의 슬라이드 1장 ≈ 뉴스 요약/뉴스 특정 단락” 등 **길이 스케일링 기준**을 설정할 계획이다.
- 이를 통해 BonCahier AI 모델이 **뉴스 기사뿐 아니라 실제 강의 슬라이드 텍스트에도 잘 동작**하도록 입력 길이, 요약 길이, 전처리 규칙을 조정할 예정이다.

---

## 8. 요약 및 향후 계획

- 한국어·영어 뉴스 요약 데이터셋을 활용하여,
  - 텍스트 길이, 압축 비율, 카테고리 분포, 문서–요약 길이 관계 등 요약 태스크의 **핵심 통계적 특성**을 파악하였다.
- 두 데이터셋을 기반으로 한 **레이블링 데이터셋(text, label)** 을 별도로 구성하여, 이후 모델 학습에서 바로 사용할 수 있는 형태로 정리하였다.
- 전체/레이블별 단어 빈도 분석을 통해
  - 각 도메인(economy, sport, politics 등)에서 **어떤 어휘가 특징적으로 등장하는지**를 확인하고,
  - 도메인별 요약 스타일 및 분류 기준을 설정할 수 있는 기초 정보를 확보하였다.
- 압축 비율 분포와 outlier 분석을 통해
  - 이후 모델 학습 전에 적용할 **필터링/정제 규칙**(최소 길이, 압축 비율 범위, 이상치 제거)을 구체화할 수 있었다.
- Assignment 5에서는 본 README에서 정리한 전처리 및 단어 분포 분석 결과를 바탕으로:
  - 정제된 한국어/영어 요약 데이터를 사용해
  - 슬라이드 요약 태스크에 활용 가능한 **기초 요약 모델(예: KoBART, BART)** 을 학습 및 평가할 예정이다.
