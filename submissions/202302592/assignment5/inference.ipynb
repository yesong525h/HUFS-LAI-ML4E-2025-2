{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "from google.colab import drive\n",
        "\n",
        "# =========================================================\n",
        "# 1. Colab 환경 설정 및 경로 정의 (필수)\n",
        "# =========================================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ⚠️ training.py에서 모델을 저장한 정확한 Google Drive 경로로 수정하세요!\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# 모델 및 토크나이저가 저장된 폴더 경로\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, \"instruction_classifier_model\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 장치: {device}\")\n",
        "\n",
        "# =========================================================\n",
        "# 2. 모델 및 토크나이저 로드\n",
        "# =========================================================\n",
        "print(f\"모델 로드 중: {MODEL_DIR}\")\n",
        "try:\n",
        "    # ⚠️ AutoTokenizer 사용 및 로컬 파일 강제 로드\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "\n",
        "    # ⚠️ 로컬 파일 강제 로드\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"모델 로드 성공.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류: 모델 로드 실패. 경로를 확인하거나 training.py 실행 여부를 확인하세요.\")\n",
        "    print(f\"상세 오류: {e}\")\n",
        "    exit()\n",
        "\n",
        "# =========================================================\n",
        "# 3. 추론 함수 정의\n",
        "# =========================================================\n",
        "def is_instruction(sentence: str) -> bool:\n",
        "    \"\"\"\n",
        "    주어진 문장이 지시문인지 (레이블 1) 아닌지 (레이블 0) 판별합니다.\n",
        "    \"\"\"\n",
        "    # 1. 토큰화 및 인코딩\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64,\n",
        "        add_special_tokens=True, # KoBERT는 CLS, SEP 토큰 필요\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # 2. GPU로 데이터 이동\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # 3. 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=mask)\n",
        "        # 로짓(Logits)에서 가장 높은 값의 인덱스(레이블)를 추출\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # 지시문 레이블이 1이라고 가정하고 True/False 반환\n",
        "    return prediction == 1\n",
        "\n",
        "# =========================================================\n",
        "# 4. 테스트\n",
        "# =========================================================\n",
        "print(\"\\n--- 테스트 시작 ---\")\n",
        "\n",
        "# 지시문으로 예상되는 문장\n",
        "test_sentence_instruction = \"다음 구절의 의미를 파악해 보자.\"\n",
        "result_instruction = is_instruction(test_sentence_instruction)\n",
        "print(f\"문장: \\\"{test_sentence_instruction}\\\" -> 지시문 여부: {result_instruction}\")\n",
        "\n",
        "# 지시문이 아닐 것으로 예상되는 문장 (예시: 평서문)\n",
        "test_sentence_statement = \"이것은 매우 중요한 개념입니다.\"\n",
        "result_statement = is_instruction(test_sentence_statement)\n",
        "print(f\"문장: \\\"{test_sentence_statement}\\\" -> 지시문 여부: {result_statement}\")\n",
        "\n",
        "# 지시문이 아닐 것으로 예상되는 문장 (예시: 질문)\n",
        "test_sentence_question = \"이 문장이 지시문인가요?\"\n",
        "result_question = is_instruction(test_sentence_question)\n",
        "print(f\"문장: \\\"{test_sentence_question}\\\" -> 지시문 여부: {result_question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVfg3QSrjtoT",
        "outputId": "0a082086-74ba-4d4b-ac05-64d3e57c5b03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "사용 장치: cuda\n",
            "모델 로드 중: /content/drive/MyDrive/Colab Notebooks/instruction_classifier_model\n",
            "모델 로드 성공.\n",
            "\n",
            "--- 테스트 시작 ---\n",
            "문장: \"다음 구절의 의미를 파악해 보자.\" -> 지시문 여부: True\n",
            "문장: \"이것은 매우 중요한 개념입니다.\" -> 지시문 여부: False\n",
            "문장: \"이 문장이 지시문인가요?\" -> 지시문 여부: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "from google.colab import drive\n",
        "import re # 문장 분리를 위해 정규표현식 라이브러리 추가\n",
        "\n",
        "# =========================================================\n",
        "# 1. Colab 환경 설정 및 경로 정의 (필수)\n",
        "# =========================================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ⚠️ training.py에서 모델을 저장한 정확한 Google Drive 경로로 수정하세요!\n",
        "# 이 경로는 input_file.txt 파일이 위치한 경로와 동일해야 합니다.\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# 모델 및 토크나이저가 저장된 폴더 경로\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, \"instruction_classifier_model\")\n",
        "INPUT_TXT_FILE = os.path.join(PROJECT_DIR, \"inf_ex1.txt\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 장치: {device}\")\n",
        "\n",
        "# =========================================================\n",
        "# 2. 모델 및 토크나이저 로드\n",
        "# =========================================================\n",
        "print(f\"모델 로드 중: {MODEL_DIR}\")\n",
        "try:\n",
        "    # ⚠️ AutoTokenizer 사용 및 로컬 파일 강제 로드\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "\n",
        "    # ⚠️ 로컬 파일 강제 로드\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"모델 로드 성공.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류: 모델 로드 실패. 경로를 확인하거나 training.py 실행 여부를 확인하세요.\")\n",
        "    print(f\"상세 오류: {e}\")\n",
        "    exit()\n",
        "\n",
        "# =========================================================\n",
        "# 3. 추론 함수 정의 (is_instruction)\n",
        "# =========================================================\n",
        "def is_instruction(sentence: str) -> bool:\n",
        "    \"\"\"\n",
        "    주어진 문장이 지시문인지 (레이블 1) 아닌지 (레이블 0) 판별합니다.\n",
        "    \"\"\"\n",
        "    # 문장이 너무 짧거나 공백이면 건너뜁니다.\n",
        "    if not sentence or len(sentence.strip()) < 3:\n",
        "        return False\n",
        "\n",
        "    # 1. 토큰화 및 인코딩\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # 2. GPU로 데이터 이동\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # 3. 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=mask)\n",
        "        # 로짓(Logits)에서 가장 높은 값의 인덱스(레이블)를 추출\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # 지시문 레이블이 1이라고 가정하고 True/False 반환\n",
        "    return prediction == 1\n",
        "\n",
        "# =========================================================\n",
        "# 4. TXT 파일 처리 및 지시문 추출 함수\n",
        "# =========================================================\n",
        "def process_txt_file(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    TXT 파일을 읽어 문장 단위로 분리하고, 지시문만 추출하여 반환합니다.\n",
        "    \"\"\"\n",
        "    extracted_instructions = []\n",
        "\n",
        "    print(f\"\\n--- 4. TXT 파일 처리 시작: {file_path} ---\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ 오류: 입력 파일을 찾을 수 없습니다. 경로: {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류: 파일 읽기 중 오류 발생: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 텍스트를 문장 단위로 분리합니다. (마침표, 물음표, 느낌표 뒤에 공백이 오는 패턴 사용)\n",
        "    # 한글 처리를 위해 복잡한 정규식 대신 일반적인 문장 부호와 re.split을 사용합니다.\n",
        "    # Note: 정확한 한국어 문장 분리는 complex!\n",
        "    sentences = re.split(r'([.?!])\\s+', text)\n",
        "\n",
        "    # re.split 결과는 문장과 구분 기호(., ?, !)가 교대로 나타나므로, 이를 다시 결합합니다.\n",
        "    processed_sentences = []\n",
        "    for i in range(0, len(sentences), 2):\n",
        "        sentence = sentences[i]\n",
        "        if i + 1 < len(sentences):\n",
        "            # 문장과 구분 기호를 합칩니다.\n",
        "            sentence += sentences[i+1]\n",
        "        processed_sentences.append(sentence.strip())\n",
        "\n",
        "\n",
        "    total_sentences = 0\n",
        "\n",
        "    # 추출된 문장들을 모델에 넣어 판별합니다.\n",
        "    for sentence in processed_sentences:\n",
        "        if sentence:\n",
        "            total_sentences += 1\n",
        "            if is_instruction(sentence):\n",
        "                extracted_instructions.append(sentence)\n",
        "\n",
        "    print(f\"총 문장 수: {total_sentences}\")\n",
        "    print(f\"지시문으로 분류된 문장 수: {len(extracted_instructions)}\")\n",
        "\n",
        "    return extracted_instructions\n",
        "\n",
        "# =========================================================\n",
        "# 5. 실행\n",
        "# =========================================================\n",
        "'''\n",
        "# 1. 테스트용 dummy TXT 파일 생성 (Colab 환경에서 테스트를 위해)\n",
        "dummy_content = \"\"\"\n",
        "다음 문장을 읽어 보세요.\n",
        "철수는 학교에 갔습니다.\n",
        "이 구절의 핵심 내용을 요약해 보자.\n",
        "오늘의 날씨는 맑을까요?\n",
        "모든 객체의 속성을 변경하세요.\n",
        "\"\"\"\n",
        "with open(INPUT_TXT_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_content)\n",
        "print(f\"더미 입력 파일 생성 완료: {INPUT_TXT_FILE}\")\n",
        "'''\n",
        "\n",
        "# 2. 파일 처리 및 지시문 추출\n",
        "instructions = process_txt_file(INPUT_TXT_FILE)\n",
        "\n",
        "# 3. 결과 출력\n",
        "print(\"\\n[ 최종 추출된 지시문 목록 ]\")\n",
        "if instructions:\n",
        "    for i, instruction in enumerate(instructions):\n",
        "        print(f\"{i+1}. {instruction}\")\n",
        "else:\n",
        "    print(\"추출된 지시문이 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xXKP7No0EGk",
        "outputId": "3ff4f0f7-2585-4b2b-d35c-3662861c4620"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "사용 장치: cuda\n",
            "모델 로드 중: /content/drive/MyDrive/Colab Notebooks/instruction_classifier_model\n",
            "모델 로드 성공.\n",
            "\n",
            "--- 4. TXT 파일 처리 시작: /content/drive/MyDrive/Colab Notebooks/inf_ex1.txt ---\n",
            "총 문장 수: 20\n",
            "지시문으로 분류된 문장 수: 5\n",
            "\n",
            "[ 최종 추출된 지시문 목록 ]\n",
            "1. 이 시를 낭독해 보고, 운율을 형성하는 요소가 무엇인지 말해 보자.\n",
            "2. ‘배려’를 주제로 한 시를 쓰고, 음악과 이미지를 넣어 영상으로 제작해 보자.\n",
            "3. 3 1\n",
            "1 에서 만든 영상 시를 사회 관계망 서비스(SNS)에 올려 친구들과 공유해 보자.\n",
            "4. ● 내가 쓴 시\n",
            "● 음악\n",
            "시와 어울리는 음악이나 효과음을 넣어 볼까?\n",
            "5. ♩♪\n",
            "♬\n",
            "● 이미지\n",
            "시와 어울리는 사진이나 동영상을 찍어 보자.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "from google.colab import drive\n",
        "import re # 문장 분리를 위해 정규표현식 라이브러리 추가\n",
        "\n",
        "# =========================================================\n",
        "# 1. Colab 환경 설정 및 경로 정의 (필수)\n",
        "# =========================================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ⚠️ training.py에서 모델을 저장한 정확한 Google Drive 경로로 수정하세요!\n",
        "# 이 경로는 input_file.txt 파일이 위치한 경로와 동일해야 합니다.\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# 모델 및 토크나이저가 저장된 폴더 경로\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, \"instruction_classifier_model\")\n",
        "INPUT_TXT_FILE = os.path.join(PROJECT_DIR, \"inf_ex2.txt\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 장치: {device}\")\n",
        "\n",
        "# =========================================================\n",
        "# 2. 모델 및 토크나이저 로드\n",
        "# =========================================================\n",
        "print(f\"모델 로드 중: {MODEL_DIR}\")\n",
        "try:\n",
        "    # ⚠️ AutoTokenizer 사용 및 로컬 파일 강제 로드\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "\n",
        "    # ⚠️ 로컬 파일 강제 로드\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"모델 로드 성공.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류: 모델 로드 실패. 경로를 확인하거나 training.py 실행 여부를 확인하세요.\")\n",
        "    print(f\"상세 오류: {e}\")\n",
        "    exit()\n",
        "\n",
        "# =========================================================\n",
        "# 3. 추론 함수 정의 (is_instruction)\n",
        "# =========================================================\n",
        "def is_instruction(sentence: str) -> bool:\n",
        "    \"\"\"\n",
        "    주어진 문장이 지시문인지 (레이블 1) 아닌지 (레이블 0) 판별합니다.\n",
        "    \"\"\"\n",
        "    # 문장이 너무 짧거나 공백이면 건너뜁니다.\n",
        "    if not sentence or len(sentence.strip()) < 3:\n",
        "        return False\n",
        "\n",
        "    # 1. 토큰화 및 인코딩\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # 2. GPU로 데이터 이동\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # 3. 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=mask)\n",
        "        # 로짓(Logits)에서 가장 높은 값의 인덱스(레이블)를 추출\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # 지시문 레이블이 1이라고 가정하고 True/False 반환\n",
        "    return prediction == 1\n",
        "\n",
        "# =========================================================\n",
        "# 4. TXT 파일 처리 및 지시문 추출 함수\n",
        "# =========================================================\n",
        "def process_txt_file(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    TXT 파일을 읽어 문장 단위로 분리하고, 지시문만 추출하여 반환합니다.\n",
        "    \"\"\"\n",
        "    extracted_instructions = []\n",
        "\n",
        "    print(f\"\\n--- 4. TXT 파일 처리 시작: {file_path} ---\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ 오류: 입력 파일을 찾을 수 없습니다. 경로: {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류: 파일 읽기 중 오류 발생: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 텍스트를 문장 단위로 분리합니다. (마침표, 물음표, 느낌표 뒤에 공백이 오는 패턴 사용)\n",
        "    # 한글 처리를 위해 복잡한 정규식 대신 일반적인 문장 부호와 re.split을 사용합니다.\n",
        "    # Note: 정확한 한국어 문장 분리는 complex!\n",
        "    sentences = re.split(r'([.?!])\\s+', text)\n",
        "\n",
        "    # re.split 결과는 문장과 구분 기호(., ?, !)가 교대로 나타나므로, 이를 다시 결합합니다.\n",
        "    processed_sentences = []\n",
        "    for i in range(0, len(sentences), 2):\n",
        "        sentence = sentences[i]\n",
        "        if i + 1 < len(sentences):\n",
        "            # 문장과 구분 기호를 합칩니다.\n",
        "            sentence += sentences[i+1]\n",
        "        processed_sentences.append(sentence.strip())\n",
        "\n",
        "\n",
        "    total_sentences = 0\n",
        "\n",
        "    # 추출된 문장들을 모델에 넣어 판별합니다.\n",
        "    for sentence in processed_sentences:\n",
        "        if sentence:\n",
        "            total_sentences += 1\n",
        "            if is_instruction(sentence):\n",
        "                extracted_instructions.append(sentence)\n",
        "\n",
        "    print(f\"총 문장 수: {total_sentences}\")\n",
        "    print(f\"지시문으로 분류된 문장 수: {len(extracted_instructions)}\")\n",
        "\n",
        "    return extracted_instructions\n",
        "\n",
        "# =========================================================\n",
        "# 5. 실행\n",
        "# =========================================================\n",
        "'''\n",
        "# 1. 테스트용 dummy TXT 파일 생성 (Colab 환경에서 테스트를 위해)\n",
        "dummy_content = \"\"\"\n",
        "다음 문장을 읽어 보세요.\n",
        "철수는 학교에 갔습니다.\n",
        "이 구절의 핵심 내용을 요약해 보자.\n",
        "오늘의 날씨는 맑을까요?\n",
        "모든 객체의 속성을 변경하세요.\n",
        "\"\"\"\n",
        "with open(INPUT_TXT_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_content)\n",
        "print(f\"더미 입력 파일 생성 완료: {INPUT_TXT_FILE}\")\n",
        "'''\n",
        "\n",
        "# 2. 파일 처리 및 지시문 추출\n",
        "instructions = process_txt_file(INPUT_TXT_FILE)\n",
        "\n",
        "# 3. 결과 출력\n",
        "print(\"\\n[ 최종 추출된 지시문 목록 ]\")\n",
        "if instructions:\n",
        "    for i, instruction in enumerate(instructions):\n",
        "        print(f\"{i+1}. {instruction}\")\n",
        "else:\n",
        "    print(\"추출된 지시문이 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxjpu9VD3Y9q",
        "outputId": "9c4572a9-cc3c-4865-cb0b-4fa870d485fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "사용 장치: cuda\n",
            "모델 로드 중: /content/drive/MyDrive/Colab Notebooks/instruction_classifier_model\n",
            "모델 로드 성공.\n",
            "\n",
            "--- 4. TXT 파일 처리 시작: /content/drive/MyDrive/Colab Notebooks/inf_ex2.txt ---\n",
            "총 문장 수: 48\n",
            "지시문으로 분류된 문장 수: 10\n",
            "\n",
            "[ 최종 추출된 지시문 목록 ]\n",
            "1. 1 사건과 등장인물을 중심으로 이 작품의 내용을 정리해 보자.\n",
            "2. ⑴ 주요 사건을 중심으로 빈칸에 들어갈 알맞은 말을 써 보자.\n",
            "3. ⑶ 등장인물의 말과 행동을 보고, 인물의 성격과 처지를 파악해 보자.\n",
            "4. ⑵ ⑴의 그림 카드 가 ~ 바 를 사건이 일어난 순서에 따라 배열해 보자.\n",
            "5. 대화를 참고하여 작품 속 갈\n",
            "등에 관한 자신의 생각을 말해 보자.\n",
            "6. ● 점순이에게 맞서지 못해서 서럽고 화가 남.\n",
            "7. ●점순이의 의도\n",
            "‘나’의 반응\n",
            "갈등의 진행\n",
            "점순이가 ‘나’에게 감자를 주었으나, ‘나’가\n",
            ".\n",
            "8. ● 점순이가 또다시 닭싸움을 붙여 ‘나’의 닭이 공격당하\n",
            "자 화가 나서\n",
            ".\n",
            "9. 한 권의 책을 읽으며 갈등을 파악해 보고\n",
            "책 전시회를 열어 볼까?\n",
            "10. 모\n",
            "둠\n",
            "⑵ 모둠에서 함께 읽을 책을 선정하고, 책을 선정한 까닭을 정리해 보자.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "from google.colab import drive\n",
        "import re # 문장 분리를 위해 정규표현식 라이브러리 추가\n",
        "\n",
        "# =========================================================\n",
        "# 1. Colab 환경 설정 및 경로 정의 (필수)\n",
        "# =========================================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ⚠️ training.py에서 모델을 저장한 정확한 Google Drive 경로로 수정하세요!\n",
        "# 이 경로는 input_file.txt 파일이 위치한 경로와 동일해야 합니다.\n",
        "PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "# 모델 및 토크나이저가 저장된 폴더 경로\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, \"instruction_classifier_model\")\n",
        "INPUT_TXT_FILE = os.path.join(PROJECT_DIR, \"inf_ex3.txt\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용 장치: {device}\")\n",
        "\n",
        "# =========================================================\n",
        "# 2. 모델 및 토크나이저 로드\n",
        "# =========================================================\n",
        "print(f\"모델 로드 중: {MODEL_DIR}\")\n",
        "try:\n",
        "    # ⚠️ AutoTokenizer 사용 및 로컬 파일 강제 로드\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "\n",
        "    # ⚠️ 로컬 파일 강제 로드\n",
        "    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"모델 로드 성공.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류: 모델 로드 실패. 경로를 확인하거나 training.py 실행 여부를 확인하세요.\")\n",
        "    print(f\"상세 오류: {e}\")\n",
        "    exit()\n",
        "\n",
        "# =========================================================\n",
        "# 3. 추론 함수 정의 (is_instruction)\n",
        "# =========================================================\n",
        "def is_instruction(sentence: str) -> bool:\n",
        "    \"\"\"\n",
        "    주어진 문장이 지시문인지 (레이블 1) 아닌지 (레이블 0) 판별합니다.\n",
        "    \"\"\"\n",
        "    # 문장이 너무 짧거나 공백이면 건너뜁니다.\n",
        "    if not sentence or len(sentence.strip()) < 3:\n",
        "        return False\n",
        "\n",
        "    # 1. 토큰화 및 인코딩\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # 2. GPU로 데이터 이동\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # 3. 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=mask)\n",
        "        # 로짓(Logits)에서 가장 높은 값의 인덱스(레이블)를 추출\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    # 지시문 레이블이 1이라고 가정하고 True/False 반환\n",
        "    return prediction == 1\n",
        "\n",
        "# =========================================================\n",
        "# 4. TXT 파일 처리 및 지시문 추출 함수\n",
        "# =========================================================\n",
        "def process_txt_file(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    TXT 파일을 읽어 문장 단위로 분리하고, 지시문만 추출하여 반환합니다.\n",
        "    \"\"\"\n",
        "    extracted_instructions = []\n",
        "\n",
        "    print(f\"\\n--- 4. TXT 파일 처리 시작: {file_path} ---\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ 오류: 입력 파일을 찾을 수 없습니다. 경로: {file_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류: 파일 읽기 중 오류 발생: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 텍스트를 문장 단위로 분리합니다. (마침표, 물음표, 느낌표 뒤에 공백이 오는 패턴 사용)\n",
        "    # 한글 처리를 위해 복잡한 정규식 대신 일반적인 문장 부호와 re.split을 사용합니다.\n",
        "    # Note: 정확한 한국어 문장 분리는 complex!\n",
        "    sentences = re.split(r'([.?!])\\s+', text)\n",
        "\n",
        "    # re.split 결과는 문장과 구분 기호(., ?, !)가 교대로 나타나므로, 이를 다시 결합합니다.\n",
        "    processed_sentences = []\n",
        "    for i in range(0, len(sentences), 2):\n",
        "        sentence = sentences[i]\n",
        "        if i + 1 < len(sentences):\n",
        "            # 문장과 구분 기호를 합칩니다.\n",
        "            sentence += sentences[i+1]\n",
        "        processed_sentences.append(sentence.strip())\n",
        "\n",
        "\n",
        "    total_sentences = 0\n",
        "\n",
        "    # 추출된 문장들을 모델에 넣어 판별합니다.\n",
        "    for sentence in processed_sentences:\n",
        "        if sentence:\n",
        "            total_sentences += 1\n",
        "            if is_instruction(sentence):\n",
        "                extracted_instructions.append(sentence)\n",
        "\n",
        "    print(f\"총 문장 수: {total_sentences}\")\n",
        "    print(f\"지시문으로 분류된 문장 수: {len(extracted_instructions)}\")\n",
        "\n",
        "    return extracted_instructions\n",
        "\n",
        "# =========================================================\n",
        "# 5. 실행\n",
        "# =========================================================\n",
        "'''\n",
        "# 1. 테스트용 dummy TXT 파일 생성 (Colab 환경에서 테스트를 위해)\n",
        "dummy_content = \"\"\"\n",
        "다음 문장을 읽어 보세요.\n",
        "철수는 학교에 갔습니다.\n",
        "이 구절의 핵심 내용을 요약해 보자.\n",
        "오늘의 날씨는 맑을까요?\n",
        "모든 객체의 속성을 변경하세요.\n",
        "\"\"\"\n",
        "with open(INPUT_TXT_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write(dummy_content)\n",
        "print(f\"더미 입력 파일 생성 완료: {INPUT_TXT_FILE}\")\n",
        "'''\n",
        "\n",
        "# 2. 파일 처리 및 지시문 추출\n",
        "instructions = process_txt_file(INPUT_TXT_FILE)\n",
        "\n",
        "# 3. 결과 출력\n",
        "print(\"\\n[ 최종 추출된 지시문 목록 ]\")\n",
        "if instructions:\n",
        "    for i, instruction in enumerate(instructions):\n",
        "        print(f\"{i+1}. {instruction}\")\n",
        "else:\n",
        "    print(\"추출된 지시문이 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlccNhPD3lCl",
        "outputId": "c9627105-6667-4628-bbc8-c52fe30bcaa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "사용 장치: cuda\n",
            "모델 로드 중: /content/drive/MyDrive/Colab Notebooks/instruction_classifier_model\n",
            "모델 로드 성공.\n",
            "\n",
            "--- 4. TXT 파일 처리 시작: /content/drive/MyDrive/Colab Notebooks/inf_ex3.txt ---\n",
            "총 문장 수: 35\n",
            "지시문으로 분류된 문장 수: 8\n",
            "\n",
            "[ 최종 추출된 지시문 목록 ]\n",
            "1. 이를 본 가람이가\n",
            "포스터에서 보완해야 할 내용을 이야기해 주는데…….\n",
            "2. (2)\n",
            "1 나리가 처음에 만든 포스터와 수정한 포스터의 차이점을 말해 보자.\n",
            "3. 2 나리와 같이 자신의 주장을 글로 써 본 경험을 이야기해 보자.\n",
            "4. 너를 부회장으로 뽑아야\n",
            "하는 타당한 근거를 들어 봐.\n",
            "5. 너의 생각, 나의 의견\n",
            "\n",
            "타당한 근거를 들어 주장하는 글은 어떻게 쓸까?\n",
            "6. 정윤이가 발견한 문제 상황을 써 보자.\n",
            "7. 개념 콕콕\n",
            "정윤이와 친구들이 쓸 글의 주장과 이유, 예상 독자를 정리해 보자.\n",
            "8. 그럼 우리가 쓸 글의 주장과 주장을 뒷받침하는 이유를 정리해 보자.\n"
          ]
        }
      ]
    }
  ]
}