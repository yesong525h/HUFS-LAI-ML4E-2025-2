{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "data_list=[\n",
        "    {\"document_id\": \"ml4e-lecture-week10\",\n",
        "    \"pages\": [\n",
        "        {   \"page_num\": 3,\n",
        "            \"source_texts\": [\n",
        "                \"Deep Learning\",\n",
        "                \"Convolutional Neural Networks\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"딥 러닝\",\n",
        "                \"컨볼루션 신경망\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True],\n",
        "        },{\n",
        "            \"page_num\": 4,\n",
        "            \"source_texts\": [\n",
        "                \"What Computers “See”? \",\n",
        "                \"Images are numbers\",\n",
        "                \"An image is just a matrix of numbers [0, 255]!\",\n",
        "                \"i.e., $1080 \\times 1080 \\times 3$ for an RGB image\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"컴퓨터가 '보는' 것은 무엇인가?\",\n",
        "                \"이미지는 숫자이다\",\n",
        "                \"이미지는 단지 [0, 255]범위의 숫자의 행렬입니다!\",\n",
        "                \"즉, RGB 이미지의 경우 $1080 \\times 1080 \\times 3$크기입니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False,False, False],\n",
        "        },{\n",
        "            \"page_num\": 5,\n",
        "            \"source_texts\": [\n",
        "                \"Tasks in Computer Vision\",\n",
        "                \"Regression: output variable takes continuous value\",\n",
        "                \" Classification: output variable takes class label. Can produce probability of belonging to a particular class\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"컴퓨터 비전의 과제\",\n",
        "                \"회귀: 출력 변수는 연속 값을 취함\",\n",
        "                \"분류: 출력 변수는 클래스 레이블을 취합니다. 특정 클래스에 속할 확률을 생성할 수 있습니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },\n",
        "        {\n",
        "            \"page_num\": 6,\n",
        "            \"source_texts\": [\n",
        "                    \"“Learning” Feature Representations\",\n",
        "                    \"Motivation\",\n",
        "                    \" The bird occupies a local area and looks the same in different parts of an image.\",\n",
        "                    \" We should construct neural networks which exploit these properties.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                    \"특징 표현의 '학습'\",\n",
        "                    \"동기\",\n",
        "                    \"새는 국소 영역을 차지하며 이미지의 다른 부분에서 동일하게 보입니다.\",\n",
        "                    \"이러한 특성을 활용하는 신경망을 구성해야 합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },\n",
        "        {\n",
        "            \"page_num\": 7,\n",
        "            \"source_texts\": [\n",
        "                \"Fully Connected Neural Network\",\n",
        "                \"Input:\",\n",
        "                \" 2D image\",\n",
        "                \" Vector of pixel values\",\n",
        "                \" Fully Connected:\",\n",
        "                \" Connect neuron in hidden layer to all neurons in input layer\",\n",
        "                \" No spatial information!\",\n",
        "                \" Spatial organization of the input is destroyed by flatten.\",\n",
        "                \" And, many, many parameters!\",\n",
        "                \" How can we use spatial structure in the input to inform the architecture of the network?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"완전 연결 신경망\",\n",
        "                \"입력:\",\n",
        "                \"2차원 이미지\",\n",
        "                \"픽셀 값 벡터\",\n",
        "                \"완전 연결:\",\n",
        "                \"숨겨진 층의 뉴런을 입력 층의 모든 뉴런에 연결\",\n",
        "                \"공간 정보 없음!\",\n",
        "                \"입력의 공간적 구성은 평탄화(flatten)로 파괴됩니다.\",\n",
        "                \"그리고, 매우 많은 매개변수!\",\n",
        "                \"입력의 공간 구조를 어떻게 활용하여 네트워크 아키텍처를 설계할 수 있을까요?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False, False, False],\n",
        "        },\n",
        "        {\n",
        "            \"page_num\": 8,\n",
        "            \"source_texts\": [\n",
        "                \"Fully Connected Layer\",\n",
        "                \"Example: 200x200 image 40K hidden units ~ 2B parameters\",\n",
        "                \" Spatial correlation is local\",\n",
        "                \" Waste of resources + we have not enough training samples anyway..\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"완전 연결된 레이어\",\n",
        "                \"예시: 200x200 이미지 40K 숨겨진 유닛 ~ 2B 매개변수\",\n",
        "                \"공간적 상관관계는 국소적\",\n",
        "                \"자원 낭비 + 어차피 충분한 훈련 샘플이 없음..\"\n",
        "            ],\n",
        "            \"needs_correction\": [True, False, False, False],\n",
        "        },\n",
        "        {\n",
        "            \"page_num\": 9,\n",
        "            \"source_texts\": [\n",
        "                \"Locally Connected Layer\",\n",
        "                \"Example: 200x200 image 40K hidden units Filter size: 10x10 4M parameters\",\n",
        "                \"Note: This parameterization is go when impur image is registere(e.face recognition).\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"국소 연결 레이어\",\n",
        "                \"예시: 200x200 이미지 40K 숨겨진 유닛 필터 크기: 10x10 4M 매개변수\",\n",
        "                \"참고: 이 매개변수화는 이미지가 정렬된 경우(예: 얼굴 인식)에 적합합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },\n",
        "        {\n",
        "            \"page_num\": 10,\n",
        "            \"source_texts\": [\n",
        "                \"Convolutional Layer\",\n",
        "                \"Share the same parameters across different locations(assuming imput is stationary): Convolutions with learned kernels\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"Convolutional Layer\",\n",
        "                \"Share the same parameters across different locations(assuming imput is stationary): Convolutions with learned kernels\"\n",
        "            ],\n",
        "            \"needs_correction\": [True, True],\n",
        "        },{\n",
        "            \"page_num\": 11,\n",
        "            \"source_texts\": [\n",
        "                \"Key Idea\",\n",
        "                \" A standard neural net applied to images:\",\n",
        "                \" Scales quadratically with the size of the input\",\n",
        "                \" Does not leverage stationarity\",\n",
        "                \" Solution:\",\n",
        "                \" Connect each hidden unit to a small patch of the input\",\n",
        "                \" Share the weight across space\",\n",
        "                \" This is called: convolutional layer.\",\n",
        "                \" A network with convolutional layers is called convolutional network.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"핵심 개념\",\n",
        "                \"이미지에 적용되는 표준 신경망:\",\n",
        "                \"입력 크기에 따라 제곱적으로 확장됨\",\n",
        "                \"정역성을 활용하지 않음\",\n",
        "                \"해결책:\",\n",
        "                \"각 숨겨진 유닛을 입력의 작은 패치에 연결\",\n",
        "                \"공간 전반에 걸쳐 가중치를 공유\",\n",
        "                \"이를 다음과 같이 부릅니다: 컨볼루션 레이어.\",\n",
        "                \" 컨볼루션 레이어를 가진 네트워크는 컨볼루션 네트워크라고 합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 12,\n",
        "            \"source_texts\": [\n",
        "                \"The Convolution Operation\",\n",
        "                \" Filter (or Kernel):\",\n",
        "                \" Discrete convolution can be viewed as element-wise multiplication by a matrix\",\n",
        "                \" Modify or enhance an image by filtering\",\n",
        "                \" Filter images to emphasize certain features or remove other features\",\n",
        "                \" Filtering includes smoothing, sharpening, and edge enhancement\",\n",
        "                \" The filter slides (or, “convolves”) across the image, repeating this operation for each overlapping region.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"컨볼루션 연산\",\n",
        "                \"필터(또는 커널):\",\n",
        "                \"이산적 합성은 행렬과의 요소별 곱셈으로 볼 수 있습니다\",\n",
        "                \"필터링을 통해 이미지를 수정하거나 향상시킵니다\",\n",
        "                \"특정 특징을 강조하거나 다른 특징을 제거하기 위해 이미지를 필터링합니다\",\n",
        "                \"필터링에는 평활화, 선명화, 가장자리 강화 등이 포함됩니다\",\n",
        "                \"필터는 이미지를 가로질러 미끄러지며(또는 “합성”하며), 겹치는 각 영역에 대해 이 작업을 반복합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 13,\n",
        "            \"source_texts\": [\n",
        "                \"Feature Extraction with Convolution: A Case Study\",\n",
        "                \"Features of X\",\n",
        "                \"Filters to detect X features\",\n",
        "                \"element-wise multiply\",\n",
        "                \"add outputs\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"컨볼루션 기반 특징 추출: 사례 연구\",\n",
        "                \"X의 특징\",\n",
        "                \"X 특징을 감지하는 필터\",\n",
        "                \"요소별 곱셈\",\n",
        "                \"출력값 합산\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 16,\n",
        "            \"source_texts\": [\n",
        "                \"Producing Feature Maps\",\n",
        "                \"Original \",\n",
        "                \"Sharpen\",\n",
        "                \"Edge detect\",\n",
        "                \"“Strong” Edge Detect\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"피처 맵 생성\",\n",
        "                \"원본 \",\n",
        "                \"선명하게\",\n",
        "                \"에지 감지\",\n",
        "                \"“강한” 에지 검출\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 17,\n",
        "            \"source_texts\": [\n",
        "                \"Pooling Layer\",\n",
        "                \"Pooling Layer\",\n",
        "                \"Let us assume filter is an 'eye' detector\",\n",
        "                \"Q.:how can we make the detection more robust to the exact location of the eye?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"풀링 레이어\",\n",
        "                \"풀링 레이어\",\n",
        "                \"필터가 '눈' 감지기라고 가정해 보자\",\n",
        "                \"Q.: 눈의 정확한 위치에 대한 감지 정확도를 어떻게 더 견고하게 만들 수 있을까?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 18,\n",
        "            \"source_texts\": [\n",
        "                \"Pooling Layer\",\n",
        "                \"Pooling Layer\",\n",
        "                \"By pooling (e.g.,taking max)filter responses at different locations we gain robustness to the exact spatiail location of features.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"풀링 레이어\",\n",
        "                \"풀링 레이어\",\n",
        "                \"풀링(예: 최대값 취하기)을 통해 서로 다른 위치의 필터 응답을 통합함으로써, 특징의 정확한 공간적 위치에 대한 강건성을 확보합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 19,\n",
        "            \"source_texts\": [\n",
        "                \"Spatial Pooling\",\n",
        "                \"Sum, average, or max\",\n",
        "                \"Non-overlapping / overlapping regions\",\n",
        "                \"Role of pooling:\",\n",
        "                \"Invariance to small transformations\",\n",
        "                \"Larger receptive fields (see more of input)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"공간 풀링\",\n",
        "                \"합계, 평균 또는 최대값\",\n",
        "                \"비중첩 / 중첩 영역\",\n",
        "                \"풀링의 역할:\",\n",
        "                \"작은 변환에 대한 불변성\",\n",
        "                \"더 큰 수용 영역(입력 더 많이 보기)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 20,\n",
        "            \"source_texts\": [\n",
        "                \"Convolutional Neural Networks\",\n",
        "                \"Feed-forward feature extraction:\",\n",
        "                \"Convolve input with learned filters: Apply filters to generate feature maps.\",\n",
        "                \"Non-linearity: Often ReLU.\",\n",
        "                \"Spatial pooling: Downsampling operation on each feature map.\",\n",
        "                \"Normalization\",\n",
        "                \"Supervised training of convolutional filters by back-propagating classification error\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"컨볼루션 신경망\",\n",
        "                \"피드포워드 특징 추출:\",\n",
        "                \"학습된 필터로 입력을 컨볼루션: 필터를 적용하여 특징 맵 생성.\",\n",
        "                \"비선형성: 종종 ReLU 사용.\",\n",
        "                \"공간 풀링: 각 특징 맵에 대한 다운샘플링 연산.\",\n",
        "                \"정규화\",\n",
        "                \"분류 오류를 역전파하여 컨볼루션 필터의 지도 학습 수행\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 21,\n",
        "            \"source_texts\": [\n",
        "                \"Important Concepts in CNN\",\n",
        "                \"1. Convolution could have multiple filters.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"CNN의 중요한 개념\",\n",
        "                \"1. 컨볼루션은 여러 필터를 가질 수 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 22,\n",
        "            \"source_texts\": [\n",
        "                \"Important Concepts in CNN\",\n",
        "                \"1. Convolution could have multiple filters.\",\n",
        "                \"2. For tensor (rank>=3), it still applies element-wise multiplication.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"CNN의 중요한 개념\",\n",
        "                \"1. 컨볼루션은 여러 필터를 가질 수 있습니다.\",\n",
        "                \"2. 텐서(랭크>=3)의 경우에도 여전히 요소별 곱셈이 적용됩니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 23,\n",
        "            \"source_texts\": [\n",
        "                \"Important Concepts in CNN\",\n",
        "                \"1. Convolution could have multiple filters.\",\n",
        "                \"2. For tensor (rank>=3), it still applies element-wise multiplication.\",\n",
        "                \"3.  Stride is the size of filter step in sliding window.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"CNN의 중요한 개념\",\n",
        "                \"1. 컨볼루션은 여러 필터를 가질 수 있습니다.\",\n",
        "                \"2. 텐서(랭크>=3)의 경우에도 여전히 요소별 곱셈이 적용됩니다.\",\n",
        "                \"3. 스트라이드는 슬라이딩 윈도우에서 필터 단계의 크기입니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True, False],\n",
        "        },{\n",
        "            \"page_num\": 24,\n",
        "            \"source_texts\": [\n",
        "                \"Important Concepts in CNN\",\n",
        "                \"1. Convolution could have multiple filters.\",\n",
        "                \"2. For tensor (rank>=3), it still applies element-wise multiplication.\",\n",
        "                \"3. Stride is the size of filter step in sliding window.\",\n",
        "                \"4. By stacking convolutional layers, we can increase receptive field.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"CNN의 중요한 개념\",\n",
        "                \"1. 컨볼루션은 여러 필터를 가질 수 있습니다.\",\n",
        "                \"2. 텐서(랭크>=3)의 경우에도 여전히 요소별 곱셈이 적용됩니다.\",\n",
        "                \"3. 스트라이드는 슬라이딩 윈도우에서 필터 단계의 크기입니다.\",\n",
        "                \"4. 컨볼루션 레이어를 쌓음으로써 수용 영역을 확대할 수 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True, False, False],\n",
        "        },{\n",
        "            \"page_num\": 25,\n",
        "            \"source_texts\": [\n",
        "                \"Important Concepts in CNN\",\n",
        "                \"1. Convolution could have multiple filters.\",\n",
        "                \"2. For tensor (rank>=3), it still applies element-wise multiplication.\",\n",
        "                \"3. Stride is the size of filter step in sliding window.\",\n",
        "                \"4. By stacking convolutional layers, we can increase receptive field.\",\n",
        "                \"5. To include pixels/neurons around the boundary of image, we need padding.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"CNN의 중요한 개념\",\n",
        "                \"1. 컨볼루션은 여러 필터를 가질 수 있습니다.\",\n",
        "                \"2. 텐서(랭크>=3)의 경우에도 여전히 요소별 곱셈이 적용됩니다.\",\n",
        "                \"3. 스트라이드는 슬라이딩 윈도우에서 필터 단계의 크기입니다.\",\n",
        "                \"4. 컨볼루션 레이어를 쌓음으로써 수용 영역을 확대할 수 있습니다.\",\n",
        "                \"5. 이미지 경계 주변의 픽셀/뉴런을 포함하려면 패딩이 필요합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 26,\n",
        "            \"source_texts\": [\n",
        "                \"Representation Learning in Deep CNNs\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"딥 CNN에서의 표현 학습\"\n",
        "            ],\n",
        "            \"needs_correction\": [False],\n",
        "        },{\n",
        "            \"page_num\": 27,\n",
        "            \"source_texts\": [\n",
        "                \"Practice 1: Feature Map Shape\",\n",
        "                \"OW, OH : output width, output height => output size\",\n",
        "                \"IW, IH : input width, input height => input size\",\n",
        "                \"FW, FH : filter width, filter height => filter size\",\n",
        "                \"P : padding size\",\n",
        "                \"S : stride\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"실습 1: 피처 맵 형태\",\n",
        "                \"OW, OH : 출력 너비, 출력 높이 => 출력 크기\",\n",
        "                \"IW, IH : 입력 너비, 입력 높이 => 입력 크기\",\n",
        "                \"FW, FH : 필터 너비, 필터 높이 => 필터 크기\",\n",
        "                \"P : 패딩 크기\",\n",
        "                \"S : 스트라이드\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 28,\n",
        "            \"source_texts\": [\n",
        "                \"Practice 1: Feature Map Shape\",\n",
        "                \"Stride = 1 (Default): Moves one pixel at a time\",\n",
        "                \"Convolution with 3x3 kernel, zero padding and stride = 1\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"실습 1: 피처 맵 형태\",\n",
        "                \"스트라이드 = 1 (기본값): 한 번에 한 픽셀씩 이동\",\n",
        "                \"3x3 커널, 제로 패딩 및 스트라이드 = 1을 사용한 컨볼루션\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 29,\n",
        "            \"source_texts\": [\n",
        "                \"Practice 1: Feature Map Shape\",\n",
        "                \"Stride > 1: Moves multiple pixels at a time → Reduces the output size, leading to downsampling.\",\n",
        "                \"Convolution with 3x3 kernel, zero padding and stride = 2\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"실습 1: 피처 맵 형태\",\n",
        "                \"스트라이드 > 1: 한 번에 여러 픽셀 이동 → 출력 크기 감소, 다운샘플링 발생\",\n",
        "                \"3x3 커널, 제로 패딩, 스트라이드 = 2를 사용한 컨볼루션\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 30,\n",
        "            \"source_texts\": [\n",
        "                \"Deep Learning\",\n",
        "                \"Recurrent Neural Networks\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"딥 러닝\",\n",
        "                \"순환 신경망\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 31,\n",
        "            \"source_texts\": [\n",
        "                \"So Far,\",\n",
        "                \"Regression, Classification, Dimension Reduction,...\",\n",
        "                \"Based on snapshot-type data\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"지금까지,\",\n",
        "                \"회귀, 분류, 차원 축소,...\",\n",
        "                \"스냅샷형 데이터 기반\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 32,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Matters\",\n",
        "                \"Given an image of a ball, can you predict where it will go next?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"순서가 중요합니다\",\n",
        "                \"공의 이미지를 주어졌을 때, 다음에 어디로 갈지 예측할 수 있나요?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 33,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Matters\",\n",
        "                \"How about this? Can you predict where it will go next?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"순서가 중요하다\",\n",
        "                \"이건 어때요? 다음에 어디로 갈지 예측할 수 있나요?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 34,\n",
        "            \"source_texts\": [\n",
        "                \"What is a Sequence?\",\n",
        "                \"Sentence\",\n",
        "                \"“This morning I took the dog for a walk.”\",\n",
        "                \"Medical signals / Speech waveform / Vibration measurement\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스란 무엇인가?\",\n",
        "                \"문장\",\n",
        "                \"“오늘 아침 나는 개를 산책시켰다.”\",\n",
        "                \"의료 신호 / 음성 파형 / 진동 측정\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 35,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Modeling\",\n",
        "                \"Sequence modeling is the task of predicting what comes next\",\n",
        "                \"E.g., “This morning I took my dog for a walk.”\",\n",
        "                \"E.g., given historical air quality, forecast air quality in next couple of hours.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 모델링\",\n",
        "                \"시퀀스 모델링은 다음에 올 것을 예측하는 작업입니다\",\n",
        "                \"예: “오늘 아침 나는 개를 산책시켰다.”\",\n",
        "                \"예: 과거 대기 질을 바탕으로 향후 몇 시간 동안의 대기 질을 예측한다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 36,\n",
        "            \"source_texts\": [\n",
        "                \"A Sequence Modeling Example: Next Word Prediction\",\n",
        "                \"Idea #1: Use a fixed window\",\n",
        "                \"Limitation: Cannot model long-term dependencies\",\n",
        "                \"E.g., “France is where I grew up, but I now live in Boston. I speak fluent ___.” \",\n",
        "                \"We need information from the distant past to accurately predict the correct word.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 모델링 예시: 다음 단어 예측\",\n",
        "                \"아이디어 #1: 고정 창 사용\",\n",
        "                \"제한점: 장기적 의존성 모델링 불가\",\n",
        "                \"예: “프랑스에서 자랐지만 지금은 보스턴에 살고 있습니다. 저는 유창한 ___를 구사합니다.”\",\n",
        "                \"정확한 단어를 예측하려면 먼 과거의 정보가 필요합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 37,\n",
        "            \"source_texts\": [\n",
        "                \"A Sequence Modeling Example: Next Word Prediction\",\n",
        "                \"Idea #2: Use entire sequence as set of counts\",\n",
        "                \" Bag-of-words model\",\n",
        "                \"Define a vocabulary and initialize a zero vector where each element represents for each word\",\n",
        "                \"Compute word frequency and update the correspond position in the vector\",\n",
        "                \"Use the vector for prediction\",\n",
        "                \"Limitation: Counts don’t preserve order\",\n",
        "                \"“The food was good, not bad at all.” vs. “The food was bad, not good at all.”\",\n",
        "                \"We need to preserve the information about order.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 모델링 예시: 다음 단어 예측\",\n",
        "                \"아이디어 #2: 전체 시퀀스를 카운트 집합으로 사용\",\n",
        "                \"단어 백 모델\",\n",
        "                \"어휘를 정의하고 각 요소가 각 단어를 나타내는 0 벡터를 초기화\",\n",
        "                \"단어 빈도를 계산하고 벡터의 해당 위치를 업데이트\",\n",
        "                \"예측에 벡터 사용\",\n",
        "                \"제한 사항: 카운트는 순서를 보존하지 않음\",\n",
        "                \"“음식이 좋았고, 전혀 나쁘지 않았어.” vs. “음식이 나빴고, 전혀 좋지 않았어.”\",\n",
        "                \"순서에 대한 정보를 보존해야 합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 38,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Modeling\",\n",
        "                \"To model sequences, we need to:\",\n",
        "                \"Handle variable-length sequences\",\n",
        "                \"Track long-term dependencies\",\n",
        "                \"Maintain information about order\",\n",
        "                \"Share parameters across the sequence\",\n",
        "                \"Solution:\",\n",
        "                \"Recurrent Neural Networks (RNNs)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 모델링\",\n",
        "                \"시퀀스를 모델링하려면 다음이 필요합니다:\",\n",
        "                \"가변 길이 시퀀스 처리\",\n",
        "                \"장기적 의존성 추적\",\n",
        "                \"순서에 대한 정보 유지\",\n",
        "                \"시퀀스 전반에 걸쳐 매개변수 공유\",\n",
        "                \"해결책:\",\n",
        "                \"순환 신경망(RNNs)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 39,\n",
        "            \"source_texts\": [\n",
        "                \"Recurrent Neural Networks (RNNs)\",\n",
        "                \"Apply a recurrence relation at every time step to process a sequence:\",\n",
        "                \"Note: the same function and set of parameters are used at every time step\",\n",
        "                \"This allows RNNs to maintain a memory of previous inputs, making them suitable for sequence modeling tasks.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"순환 신경망(RNNs)\",\n",
        "                \"시퀀스를 처리하기 위해 매 시간 단계마다 재귀 관계를 적용합니다:\",\n",
        "                \"참고:  매 시간 단계마다 동일한 함수와 매개변수 집합이 사용됩니다\",\n",
        "                \"이를 통해 RNN은 이전 입력에 대한 기억을 유지할 수 있어 시퀀스 모델링 작업에 적합합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False]\n",
        "        },{\n",
        "            \"page_num\": 40,\n",
        "            \"source_texts\": [\n",
        "                \"Standard Feed-Forward Neural Network\",\n",
        "                \"One to One\",\n",
        "                \"\\\"Vanilla\\\" neural network\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"표준 전방전달 신경망\",\n",
        "                \"일대일\",\n",
        "                \"기본 신경망\"\n",
        "            ],\n",
        "            \"needs_correction\": [True, False, False]\n",
        "        },{\n",
        "            \"page_num\": 41,\n",
        "            \"source_texts\": [\n",
        "                \"Recurrent Neural Networks\",\n",
        "                \"One to Many, Many to One, Many to Many\",\n",
        "                \"… and many other architectures and applications\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"순환 신경망(RNNs)\",\n",
        "                \"일대다, 다대일, 다대다\",\n",
        "                \"... 그리고 기타 다양한 아키텍처와 응용 분야\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 42,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: State Update and Output\",\n",
        "                \"Update hidden state, $f_w$\",\n",
        "                \"Compute output vector\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 상태 업데이트 및 출력\",\n",
        "                \"숨겨진 상태 업데이트, $f_w$\",\n",
        "                \"출력 벡터 계산\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False],\n",
        "        },{\n",
        "            \"page_num\": 43,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: Computational Graph across Time\",\n",
        "                \"Represent as computational graph unrolled across time\",\n",
        "                \"It uses gates to control the flow of information, allowing it to maintain a memory cell over long sequences.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 시간에 따른 계산 그래프\",\n",
        "                \"시간에 따라 펼쳐진 계산 그래프로 표현\",\n",
        "                \"정보 흐름을 제어하기 위해 게이트를 사용하여 긴 시퀀스 동안 메모리 셀을 유지할 수 있게 합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 47,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: Computational Graph across Time\",\n",
        "                \"Re-use the same weight matrices at every time step\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 시간에 따른 계산 그래프\",\n",
        "                \"모든 시간 단계에서 동일한 가중치 행렬을 재사용합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 48,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: Computational Graph across Time\",\n",
        "                \"Compute the loss $\\mathcal{L}_t$ by comparing $\\hat{y}_t$ and $y_t$ ($y_t$ is ground truth)\",\n",
        "                \"e.g., $\\mathcal{L}_t = (\\hat{y}_t - y_t)^2$\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 시간에 따른 계산 그래프\",\n",
        "                \"$\\hat{y}_t$와 $y_t$($y_t$는 실제 값)를 비교하여 손실 $\\mathcal{L}_t$를 계산합니다\",\n",
        "                \"예: $\\mathcal{L}_t = (\\hat{y}_t - y_t)^2$\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 49,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: Computational Graph across Time\",\n",
        "                \"Total loss $\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t$\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 시간에 따른 계산 그래프\",\n",
        "                \"전체 손실 $\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t$\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 50,\n",
        "            \"source_texts\": [\n",
        "                \"RNN: Backpropagation Through Time\",\n",
        "                \"For backpropagation, we need to compute the gradients w.r.t. $W_{hy}$, $W_{hh}$, and $W_{xh}$.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN: 시간에 따른 역전파\",\n",
        "                \"역전파를 위해서는 $W_{hy}$, $W_{hh}$, 및 $W_{xh}$에 대한 기울기를 계산해야 합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 51,\n",
        "            \"source_texts\": [\n",
        "                \"Standard RNN Gradient Flow\",\n",
        "                \"Computing the gradient w.r.t. $h_0$ involves many factors of $W_{hh}$ + repeated gradient computation!\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"표준 RNN 기울기 흐름\",\n",
        "                \"$h_0$에 대한 기울기 계산에는 $W_{hh}$의 많은 요소와 반복된 기울기 계산이 포함됩니다!\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 52,\n",
        "            \"source_texts\": [\n",
        "                \"Standard RNN Gradient Flow: Exploding Gradients\",\n",
        "                \"Computing the gradient w.r.t. $h_0$ involves many factors of $W_{hh}$ + repeated gradient computation!\",\n",
        "                \"Many values > 1: exploding gradients, Gradient clipping to scale big gradients\",\n",
        "                \"Many values < 1: vanishing gradients, 1. Activation function, 2. Weight initialization, 3. Network architecture\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"표준 RNN 기울기 흐름: 폭발하는 기울기\",\n",
        "                \"$h_0$에 대한 기울기 계산에는 $W_{hh}$의 여러 요소와 반복된 기울기 계산이 포함됩니다!\",\n",
        "                \"1보다 큰 값이 많음: 폭발하는 기울기, 큰 기울기를 조정하기 위한 기울기 클리핑\",\n",
        "                \"1보다 큰 값이 많을 경우: 폭발하는 기울기, 1. 활성화 함수, 2. 가중치 초기화, 3. 네트워크 구조\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 54,\n",
        "            \"source_texts\": [\n",
        "                \"Why is Exploding Gradient a Problem? \",\n",
        "                \" If the gradient becomes too big, then SGD update step becomes too big:\",\n",
        "                \"This can be bad updates: we take too large a step and reach a weird and bad parameter configuration (with large loss)\",\n",
        "                \"You think you’ve found a hill to climb, but suddenly you’re in Iowa.\",\n",
        "                \"In the worst case, this will result in Inf or NaNin your network (then you have to restart training from an earlier checkpoint)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"왜 폭발하는 기울기가 문제인가?\",\n",
        "                \"기울기가 너무 커지면 SGD 업데이트 단계가 너무 커집니다:\",\n",
        "                \"이는 나쁜 업데이트가 될 수 있습니다: 너무 큰 단계를 밟아 이상하고 나쁜 매개변수 구성(큰 손실)에 도달하게 됩니다\",\n",
        "                \"오를 언덕을 찾았다고 생각했는데, 갑자기 아이오와에 있는 자신을 발견하게 됩니다.\",\n",
        "                \"최악의 경우, 네트워크에서 무한대(Inf)나 무한대(NaN)가 발생할 수 있습니다(그러면 이전 체크포인트에서 훈련을 다시 시작해야 합니다)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 55,\n",
        "            \"source_texts\": [\n",
        "                \"Solution: Gradient Clipping\",\n",
        "                \"Gradient clipping: if the norm of the gradient is greater than some threshold, scale it down before applying SGD update\",\n",
        "                \"Intuition: take a step in the same direction, but a smaller step\",\n",
        "                \"In practice: remembering to clip gradients is important, but exploding gradients are an easy problem to solve.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"해결책: 기울기 클리핑\",\n",
        "                \"경사 클리핑: 경사의 노름이 특정 임계값을 초과할 경우, SGD 업데이트 적용 전에 축소합니다\",\n",
        "                \"직관: 동일한 방향으로 한 걸음 나아가되, 더 작은 걸음으로\",\n",
        "                \"실제 적용: 경사 클리핑을 기억하는 것이 중요하지만, 폭발하는 경사는 해결하기 쉬운 문제입니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 56,\n",
        "            \"source_texts\": [\n",
        "                \"The Problem of Long-Term Dependencies\",\n",
        "                \"Why are vanishing gradients a problem?\",\n",
        "                \"Multiply many small numbers together\",\n",
        "                \"Errors due to further back time steps have smaller and smaller gradients\",\n",
        "                \"Bias parameters to capture short-term dependencies\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"장기 의존성의 문제\",\n",
        "                \"사라지는 기울기가 왜 문제인가?\",\n",
        "                \"많은 작은 숫자들을 곱하기\",\n",
        "                \"더 먼 시간 단계의 오류는 점점 더 작은 기울기를 가진다\",\n",
        "                \"단기 의존성을 포착하기 위한 편향 매개변수\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 57,\n",
        "            \"source_texts\": [\n",
        "                \"Vanishing Gradient Intuition\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"소멸하는 기울기의 직관적 이해\"\n",
        "            ],\n",
        "            \"needs_correction\": [False],\n",
        "        },{\n",
        "            \"page_num\": 60,\n",
        "            \"source_texts\": [\n",
        "                \"Vanishing Gradient Intuition\",\n",
        "                \"Vanishing gradient problem: When these are small, the gradient signal gets smaller and smaller as it backpropagates further\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"소멸하는 기울기의 직관적 이해\",\n",
        "                \"소멸하는 기울기 문제: 이 값들이 작을 때, 기울기 신호는 역전파가 진행될수록 점점 더 작아진다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 62,\n",
        "            \"source_texts\": [\n",
        "                \"Why is Vanishing Gradient a Problem? \",\n",
        "                \"Gradient signal from far away is lost because it’s much smaller than gradient signal from close-by.\",\n",
        "                \"So, model weights are basically updated only with respect to near effects, not long-term effects.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"소멸하는 기울기가 문제인 이유는 무엇인가요?\",\n",
        "                \"먼 곳에서 오는 기울기 신호는 가까운 곳에서 오는 신호보다 훨씬 작기 때문에 손실됩니다.\",\n",
        "                \"따라서 모델 가중치는 기본적으로 장기적 영향이 아닌 근접한 영향에 대해서만 업데이트됩니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 63,\n",
        "            \"source_texts\": [\n",
        "                \"Why is Vanishing Gradient a Problem? \",\n",
        "                \"LM task: When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her _______.\",\n",
        "                \"To learn from this example, the RNN-LM needs to model the dependency between “tickets” on the 7th step and the target ord “tickets” at the end. \",\n",
        "                \" But if the gradient is small, the model can’t learn this dependency.\",\n",
        "                \" So, the model is unable to predict similar long-distance dependencies at test time.\",\n",
        "                \" In practice, a simple RNN will only condition ~7 tokens back. \"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"소멸하는 기울기(Vanishing Gradient)가 문제인 이유는 무엇인가요?\",\n",
        "                \"LM 작업: 그녀가 티켓을 인쇄하려고 했을 때, 프린터의 토너가 다 떨어진 것을 발견했습니다. 그녀는 문구점에 가서 토너를 더 사러 갔습니다. 가격이 매우 비쌌습니다. 프린터에 토너를 설치한 후, 그녀는 마침내 티켓을 인쇄했습니다.\",\n",
        "                \"이 예에서 학습하려면 RNN-LM은 7번째 단계의 ‘tickets'와 마지막 목표 단어 'tickets’ 사이의 의존성을 모델링해야 합니다.\",\n",
        "                \"그러나 기울기가 작으면 모델은 이 의존성을 학습할 수 없습니다.\",\n",
        "                \"따라서 모델은 테스트 시 유사한 장거리 의존성을 예측할 수 없습니다.\",\n",
        "                \"실제로, 단순한 RNN은 약 7개의 토큰만 조건부로 전달합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 64,\n",
        "            \"source_texts\": [\n",
        "                \"Solution: Gating Mechanisms in Neurons\",\n",
        "                \"Use a more complex recurrent unit with gates to control what information is passed through\",\n",
        "                \"Long Short-Term Memory (LSTM) networks rely on gated cells to track information throughout many time steps.\",\n",
        "                \"Intuition: could we design an RNN with separate memory which is added to?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"해결책: 뉴런의 게이트 메커니즘\",\n",
        "                \"정보 전달을 제어하는 게이트가 있는 더 복잡한 재귀 유닛 사용\",\n",
        "                \"장단기 메모리(LSTM) 네트워크는 여러 시간 단계에 걸쳐 정보를 추적하기 위해 게이트 셀에 의존합니다.\",\n",
        "                \"직관: 추가되는 별도의 메모리를 가진 RNN을 설계할 수 있을까?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 65,\n",
        "            \"source_texts\": [\n",
        "                \"Standard RNNs\",\n",
        "                \"In a standard RNN, recurrent modules contain simple computation\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"표준 RNN\",\n",
        "                \"표준 RNN에서 재귀 모듈은 간단한 연산을 포함합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 66,\n",
        "            \"source_texts\": [\n",
        "                \"Long Short-Term Memory (LSTM)\",\n",
        "                \"In an LSTM network, recurrent modules contain gated cells that control the information flow\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"Long Short-Term Memory (LSTM)\",\n",
        "                \"In an LSTM network, recurrent modules contain gated cells that control the information flow\"\n",
        "            ],\n",
        "            \"needs_correction\": [True, True],\n",
        "        },{\n",
        "            \"page_num\": 67,\n",
        "            \"source_texts\": [\n",
        "                \"Long Short-Term Memory (LSTM)\",\n",
        "                \"Besides hidden state $h_t$ (same as RNN), LSTM maintains a cell state $c_t$, where it’s easy for information to flow.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"장단기 메모리(LSTM)\",\n",
        "                \"은닉 상태 $h_t$ (RNN과 동일) 외에도, LSTM은 정보 흐름이 용이한 셀 상태 $c_t$를 유지합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 68,\n",
        "            \"source_texts\": [\n",
        "                \"Long Short-Term Memory (LSTM)\",\n",
        "                \"Information is added or removed to cell state through structures called gates.\",\n",
        "                \"Gates optionally let information through, via a sigmoid layer and pointwise multiplication\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"장단기 기억(LSTM)\",\n",
        "                \"정보는 게이트라고 불리는 구조를 통해 셀 상태에 추가되거나 제거됩니다.\",\n",
        "                \"게이트는 시그모이드 층과 점별 곱셈을 통해 선택적으로 정보를 통과시킵니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [True, False, True],\n",
        "        },{\n",
        "            \"page_num\": 69,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Forget Irrelevant Information\",\n",
        "                \"Concatenate previous hidden state and current input\",\n",
        "                \"When $\\sigma$ outputs 0, the network will “completely forget” the information from $c_{t-1}$.\",\n",
        "                \"When $\\sigma$ outputs 1, “completely keep”\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 불필요한 정보 잊기\",\n",
        "                \"이전 숨겨진 상태와 현재 입력을 연결\",\n",
        "                \"$\\sigma$가 0을 출력하면 네트워크는 $c_{t-1}$의 정보를 '완전히 잊어버린다.'\",\n",
        "                \"$\\sigma$가 1을 출력하면 '완전히 유지'합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 70,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Add New Information\",\n",
        "                \"$\\sigma$ decides what values to update\",\n",
        "                \"$\\tanh$ generates “candidate values” that could be added to cell state\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 새로운 정보 추가\",\n",
        "                \"$\\sigma$는 업데이트할 값을 결정합니다\",\n",
        "                \"$\\tanh$는 셀 상태에 추가될 수 있는 '후보 값'을 생성합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 71,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Update Cell State\",\n",
        "                \"$f_t * c_{t-1}$ is to apply forget gate to previous cell state\",\n",
        "                \"$i_t * \\tilde{c}_t$ is to add new candidate values to cell state\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 셀 상태 업데이트\",\n",
        "                \"$f_t * c_{t-1}$은 이전 셀 상태에 잊음 게이트를 적용하는 것\",\n",
        "                \"$i_t * \\tilde{c}_t$는 새로운 후보 값을 셀 상태에 추가하는 것\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 72,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Output Filtered Version of Cell State\",\n",
        "                \"$\\sigma$ decides what parts of the cell state to output as current hidden state\",\n",
        "                \"$\\tanh$ squashes values between -1 and 1\",\n",
        "                \"$o_t * \\tanh(c_t)$ is to output filtered version of cell\",\n",
        "                \"$h_t$ will be used to compute $\\hat{y}_t$\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 필터링된 셀 상태 버전 출력\",\n",
        "                \"$\\sigma$는 셀 상태의 어떤 부분을 현재 숨겨진 상태로 출력할지 결정합니다\",\n",
        "                \"$\\tanh$는 값을 -1과 1 사이로 압축합니다\",\n",
        "                \"$o_t * \\tanh(c_t)$는  필터링된 셀 버전을 출력하기 위한 것입니다 \",\n",
        "                \"$h_t$는 $\\hat{y}_t$를 계산하는 데 사용될 것입니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 73,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Cell State Matters\",\n",
        "                \"The information flows through cell state $c_t$.\",\n",
        "                \"You can think of the LSTM equations visually like this:\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 셀 상태의 중요성\",\n",
        "                \"정보는 셀 상태 $c_t$를 통해 흐릅니다.\",\n",
        "                \"LSTM 방정식을 시각적으로 이렇게 생각할 수 있습니다:\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 77,\n",
        "            \"source_texts\": [\n",
        "                \"LSTM: Key Concepts\",\n",
        "                \" Maintain a separate cell state from what is outputted\",\n",
        "                \" Use gates to control the flow of information\",\n",
        "                \" Forget gate gets rid of irrelevant information\",\n",
        "                \" Selectively updates cell state\",\n",
        "                \" Output gate returns a filtered version of the cell state\",\n",
        "                \" LSTM can mitigate vanishing gradient problem\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"LSTM: 핵심 개념\",\n",
        "                \"출력되는 내용과 별개의 셀 상태를 유지합니다\",\n",
        "                \"게이트를 사용하여 정보 흐름을 제어합니다\",\n",
        "                \"잊기 게이트는 관련 없는 정보를 제거합니다\",\n",
        "                \"셀 상태를 선택적으로 업데이트합니다\",\n",
        "                \"출력 게이트는 필터링된 버전의 셀 상태를 반환합니다\",\n",
        "                \"LSTM은 소멸하는 기울기 문제를 완화할 수 있습니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 78,\n",
        "            \"source_texts\": [\n",
        "                \"Supplementary: Residual Connection\",\n",
        "                \" Is vanishing/exploding gradient just an RNN problem?\",\n",
        "                \" No! It can be a problem for all neural architectures (including feed-forward and convolutional neural networks), especially very deep ones.\",\n",
        "                \" Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates\",\n",
        "                \" Thus, lower layers are learned very slowly (i.e., are hard to train)\",\n",
        "                \" Another solution: lots of new deep feedforward/convolutional architectures add more direct connections (thus allowing the gradient to flow)\",\n",
        "                \" For example:\",\n",
        "                \" Residual connects a.k.a. “ResNet”\",\n",
        "                \" Also known as skip-connections\",\n",
        "                \" The identity connection preserves information by default\",\n",
        "                \" This makes deep networks much easier to train\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"보충 설명: 잔차 연결\",\n",
        "                \"소멸/폭발하는 기울기는 RNN만의 문제인가?\",\n",
        "                \"아니다! 이는 모든 신경망 구조(피드포워드 및 컨볼루션 신경망 포함), 특히 매우 깊은 구조에서 문제가 될 수 있다.\",\n",
        "                \"연쇄 법칙/비선형 함수의 선택으로 인해, 역전파 과정에서 기울기가 소멸적으로 작아질 수 있습니다\",\n",
        "                \"따라서 하위 계층은 매우 느리게 학습됩니다 (즉, 훈련이 어렵습니다)\",\n",
        "                \"또 다른 해결책: 많은 새로운 심층 전이/컨볼루션 아키텍처는 더 많은 직접 연결을 추가합니다 (이로써 기울기가 흐를 수 있게 함)\",\n",
        "                \"예를 들어:\",\n",
        "                \"잔차 연결 일명 'ResNet'\",\n",
        "                \"스킵 연결(skip-connections)이라고도 함\",\n",
        "                \"동일 연결(identity connection)은 기본적으로 정보를 보존함\",\n",
        "                \"이로 인해 심층 네트워크 훈련이 훨씬 용이해짐\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 79,\n",
        "            \"source_texts\": [\n",
        "                \"RNN Applications\",\n",
        "                \"RNNs can be used for sequence tagging\",\n",
        "                \"e.g., part-of-speech tagging, named entity recognition\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN 응용 분야\",\n",
        "                \"RNN은 시퀀스 태깅에 활용될 수 있음\",\n",
        "                \"예: 품사 태깅, 명명된 엔티티 인식\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 80,\n",
        "            \"source_texts\": [\n",
        "                \"RNN Applications\",\n",
        "                \"RNNs can be used as a sentence encoder model\",\n",
        "                \"e.g., sentiment classification\",\n",
        "                \"How to compute sentence encoding?\",\n",
        "                \"Basic way: use final hidden state\",\n",
        "                \"Usually better: Take element-wise max or mean of all hidden states\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN 응용\",\n",
        "                \"RNN은 문장 인코더 모델로 활용될 수 있습니다\",\n",
        "                \"예: 감정 분류\",\n",
        "                \"문장 인코딩은 어떻게 계산하나요?\",\n",
        "                \"기본 방법: 최종 숨겨진 상태 사용\",\n",
        "                \"일반적으로 더 우수: 모든 숨겨진 상태의 요소별 최대값 또는 평균값 취하기\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 83,\n",
        "            \"source_texts\": [\n",
        "                \"RNN Applications\",\n",
        "                \"RNNs can be used to generate text based on other information\",\n",
        "                \"e.g., speech recognition, machine translation, summarization\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN 응용 분야\",\n",
        "                \"RNN은 다른 정보를 기반으로 텍스트를 생성하는 데 사용될 수 있습니다\",\n",
        "                \"예: 음성 인식, 기계 번역, 요약\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 85,\n",
        "            \"source_texts\": [\n",
        "                \"Bidirectional and Multi-layer RNNs\",\n",
        "                \"Motivation\",\n",
        "                \"We can regard this hidden state as a representation of the word “terribly” in the context of this sentence. We call this a contextual representation.\",\n",
        "                \"These contextual representations only contain information about the left context (e.g., “the movie was”).\",\n",
        "                \"What about right context?\",\n",
        "                \"In this example, “exciting” is in the right context and this modifies the meaning of “terribly” (from negative to positive)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"양방향 및 다층 RNN\",\n",
        "                \"동기\",\n",
        "                \"이 숨겨진 상태를 이 문맥에서 'terribly'라는 단어의 표현으로 볼 수 있습니다. 이를 문맥적 표현이라고 부릅니다.\",\n",
        "                \"이러한 맥락적 표현은 왼쪽 문맥(예: 'the movie was')에 대한 정보만 포함합니다.\",\n",
        "                \"오른쪽 문맥은 어떨까요?\",\n",
        "                \"이 예시에서 'exciting'은 오른쪽 문맥에  위치하며, 이는 “terribly”의 의미를 부정적에서 긍정적으로 수정합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 86,\n",
        "            \"source_texts\": [\n",
        "                \"Bidirectional and Multi-layer RNNs\",\n",
        "                \"This contextual representation of “terribly” has both left and right context!\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"양방향 및 다층 RNN\",\n",
        "                \"이 'terribly'의 문맥 표현은 좌측과 우측 문맥을 모두 갖습니다!\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 87,\n",
        "            \"source_texts\": [\n",
        "                \"Bidirectional RNNs\",\n",
        "                \"This is a general notation to mean “compute one forward step of the RNN” :it could be a simple RNN or LSTM computation.\",\n",
        "                \"We regard this as “the hidden state” of a bidirectional RNN\",\n",
        "                \"This is what we pass on the next parts of the network.\",\n",
        "                \"Generally, these two RNNs have separate weights\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"양방향 RNN\",\n",
        "                \"이는 RNN의 한 단계 전진 계산을 수행한다'는 일반적인 표기법입니다: 단순 RNN이나 LSTM 계산일 수 있습니다.\",\n",
        "                \"우리는 이를 양방향 RNN의 '은닉 상태'로 간주합니다\",\n",
        "                \"이것이 네트워크의 다음 부분으로 전달되는 내용입니다.\",\n",
        "                \"일반적으로 이 두 RNN은 별도의 가중치를 가집니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 88,\n",
        "            \"source_texts\": [\n",
        "                \"Bidirectional RNNs: Simplified Diagram\",\n",
        "                \"The two-way arrows indicate bidirectionality and the depicted hidden states are assumed to be the concatenated forwards+backwards states\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"양방향 RNN: 간단한 다이어그램\",\n",
        "                \"양방향 화살표는 양방향성을 나타내며, 표시된 숨겨진 상태는 전방+후방 상태를 연결한 것으로 가정합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 89,\n",
        "            \"source_texts\": [\n",
        "                \"Bidirectional RNNs\",\n",
        "                \"Note: bidirectional RNNs are only applicable if you have access to the entire input sequence\",\n",
        "                \"They are not applicable to Language Modeling, because in LM you only have left context available.\",\n",
        "                \"If you do have entire input sequence (e.g., any kind of encoding), bidirectionality is powerful (you should use it by default).\",\n",
        "                \"For example, BERT (Bidirectional Encoder Representations from Transformers) is a powerful pretrained contextual representation system built on bidirectionality.\",\n",
        "                \"You will learn more about transformers, including BERT, in a couple of weeks!\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"양방향 RNN\",\n",
        "                \"참고: 양방향 RNN은 전체 입력 시퀀스에 접근할 수 있는 경우에만 적용 가능합니다\",\n",
        "                \"언어 모델링(LM)에는 적용할 수 없습니다. LM에서는 왼쪽 컨텍스트만 사용할 수 있기 때문입니다.\",\n",
        "                \"전체 입력 시퀀스(예: 모든 종류의 인코딩)가 있다면 양방향성은 강력합니다(기본적으로 사용해야 합니다).\",\n",
        "                \"예를 들어, BERT(Bidirectional Encoder Representations from Transformers)는 양방향성을 기반으로 구축된 강력한 사전 훈련된 문맥 표현 시스템입니다.\",\n",
        "                \"BERT를 포함한 트랜스포머에 대해 몇 주 후에 더 자세히 배우게 될 것입니다!\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 90,\n",
        "            \"source_texts\": [\n",
        "                \"Multi-layer RNNs\",\n",
        "                \"RNNs are already “deep” on one dimension (they unroll over many timesteps)\",\n",
        "                \"We can also make them “deep” in another dimension by applying multiple RNNs: this is a multi-layer RNN.\",\n",
        "                \"This allows the network to compute more complex representations\",\n",
        "                \"The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features.\",\n",
        "                \"Multi-layer RNNs are also called stacked RNNs\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"다중층 RNN\",\n",
        "                \"RNN은 이미 한 차원에서 ‘깊다’(여러 시간 단계에 걸쳐 전개됨)\",\n",
        "                \"여러 RNN을 적용하여 다른 차원에서도  ‘깊게’ 만들 수 있다: 이것이 다중층 RNN이다.\",\n",
        "                \"이를 통해 네트워크는 더 복잡한 표현을 계산할 수 있습니다\",\n",
        "                \"하위 RNN은 하위 레벨 특징을, 상위 RNN은 상위 레벨 특징을 계산해야 합니다.\",\n",
        "                \"다중 레이어 RNN은 스택형 RNN이라고도 합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 92,\n",
        "            \"source_texts\": [\n",
        "                \"Multi-layer RNNs\",\n",
        "                \"Multi-layer or stacked RNNs allow a network to compute more complex representations: they work better than just one layer of high-dimensional encodings!\",\n",
        "                \"The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features.\",\n",
        "                \"High-performing RNNs are usually multi-layer (but aren’t as deep as convolutional or feed-forward networks)\",\n",
        "                \"For example: In a 2017 paper, Britz et al. find that for Neural Machine Translation, 2 to 4 layers is best for encoder RNN, and 4 layers is best for the decoder RNN\",\n",
        "                \"Often 2 layers is a lot better than 1, and 3 might be a little better than 2\",\n",
        "                \"Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g., 8 layers)\",\n",
        "                \"Transformer-based networks (e.g., BERT) are usually deeper, like 12 or 24 layers.\",\n",
        "                \"You will learn about Transformers later; they have a lot of skipping-like connections.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"다중 레이어 RNN\",\n",
        "                \"다중 레이어 또는 스택형 RNN은 네트워크가 더 복잡한 표현을 계산할 수 있게 합니다: 고차원 인코딩의 단일 레이어보다 더 효과적으로 작동합니다!\",\n",
        "                \"하위 RNN은 하위 수준의 특징을 계산해야 하며 상위 RNN은 상위 수준의 특징을 계산해야 합니다.\",\n",
        "                \"고성능 RNN은 일반적으로 다중레이어 구조를 가집니다(하지만 컨볼루션 네트워크나 피드포워드 네트워크만큼 깊지는 않습니다)\",\n",
        "                \"예를 들어: 2017년 논문에서 Britz 등은 신경망 기계 번역(NMT)의 경우 인코더 RNN에는 2~4층이 최적이며 디코더 RNN에는 4층이 최적이라고 밝혔습니다\",\n",
        "                \"종종 2층이 1층보다 훨씬 우수하며, 3층은 2층보다 약간 더 나을 수 있습니다\",\n",
        "                \"일반적으로 더 깊은 RNN(예: 8층)을 훈련하려면 스킵 연결/밀집 연결이 필요합니다\",\n",
        "                \"트랜스포머 기반 네트워크(예: BERT)는 일반적으로 12층 또는 24층처럼 더 깊습니다.\",\n",
        "                \"트랜스포머에 대해서는 나중에 배우게 될 것입니다; 스킵 연결과 유사한 연결이 많이 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 93,\n",
        "            \"source_texts\": [\n",
        "                \"RNN Limitations\",\n",
        "                \" Limitations\",\n",
        "                \" Encoding bottleneck: Fixed-size hidden state → information loss\",\n",
        "                \" Slow, no parallelization: Step-by-step processing → slow on long sequences\",\n",
        "                \" Not long memory: Vanishing/exploding gradients → weak long-term dependency\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"RNN 한계점\",\n",
        "                \"한계\",\n",
        "                \"인코딩 병목 현상: 고정 크기 숨겨진 상태 → 정보 손실\",\n",
        "                \"느림, 병렬화 불가: 단계별 처리 → 긴 시퀀스에서 느림\",\n",
        "                \"장기 기억 부족: 소멸/폭발하는 기울기 → 약한 장기 의존성\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        }\n",
        "    ]\n",
        "},{\n",
        "    \"document_id\": \"ml4e-lecture-week9\",\n",
        "    \"pages\": [\n",
        "        {\n",
        "            \"page_num\": 2,\n",
        "            \"source_texts\": [\n",
        "                \"Unsupervised Learning\",\n",
        "                \"Dimensionality Reduction: Principal Component Analysis (PCA)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"비지도 학습\",\n",
        "                \"차원 축소: 주성분 분석(PCA)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False],\n",
        "        },{\n",
        "            \"page_num\": 3,\n",
        "            \"source_texts\": [\n",
        "                \"Why reduce dimensions?\",\n",
        "                \"High dimensionality has many costs (a.k.a. Curse of Dimensionality)\",\n",
        "                \"Computation may become infeasible\",\n",
        "                \"What if your algorithm scales as O(n^3)?\",\n",
        "                \"Redundant and irrelevant features degrade performance of some ML algorithms\",\n",
        "                \"Difficulty in interpretation and visualization\",\n",
        "                \"Storage requirements getting bigger\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"왜 차원을 줄여야 할까?\",\n",
        "                \"고차원성은 많은 비용을 초래합니다(일명 차원의 저주)\",\n",
        "                \"계산이 불가능해질 수 있습니다\",\n",
        "                \"알고리즘이 O(n^3)로 확장된다면?\",\n",
        "                \"중복되고 관련 없는 특징은 일부 머신러닝 알고리즘의 성능을 저하시킵니다\",\n",
        "                \"해석과 시각화의 어려움\",\n",
        "                \"저장 공간 요구량이 커집니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 4,\n",
        "            \"source_texts\": [\n",
        "                \"Principal Component Analysis (PCA)\",\n",
        "                \" The 1st PC is the projection direction that maximizes the variance of the projected data\",\n",
        "                \" The 2nd PC is the projection direction that is orthogonal to the 1st PC and maximizes the variance\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"주성분 분석(PCA)\",\n",
        "                \"1차 주성분은 투영된 데이터의 분산을 최대화하는 투영 방향입니다\",\n",
        "                \"2차 주성분은 1차 주성분에 직교하면서 분산을 최대화하는 투영 방향입니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 5,\n",
        "            \"source_texts\": [\n",
        "                \"Principal Component Analysis (PCA)\",\n",
        "                \"Principal components are linear combinations of the original variables that have the maximum variance compared to other linear combinations.\",\n",
        "                \"Essentially, these components capture as much information from the original datasets as possible.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"주성분 분석(PCA)\",\n",
        "                \"주성분은 다른 선형 조합에 비해 최대 분산을 가지는 원본 변수의 선형 조합입니다.\",\n",
        "                \"본질적으로, 이러한 성분들은 원본 데이터 세트에서 가능한 한 많은 정보를 포착합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 7,\n",
        "            \"source_texts\": [\n",
        "                \"PCA: Conceptual Algorithm\",\n",
        "                \" 1. Maximize variance (most separable)\",\n",
        "                \" 2. Minimize the sum-of-squares (minimum squared error)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"PCA: 개념적 알고리즘\",\n",
        "                \"1. 분산 극대화 (최대 분리 가능성)\",\n",
        "                \"2. 제곱합 최소화 (최소 제곱 오차)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 8,\n",
        "            \"source_texts\": [\n",
        "                \"PCA Algorithm: Pre-processing\",\n",
        "                \" Given data\",\n",
        "                \" Shifting (zero mean) and rescaling (unit variance)\",\n",
        "                \"1) Shift to zero mean\",\n",
        "                \"2) [optional] Rescaling (unit variance)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"PCA 알고리즘: 전처리\",\n",
        "                \"주어진 데이터\",\n",
        "                \"이동(평균 0) 및 재스케일링(단위 분산)\",\n",
        "                \"1) 평균 0으로 이동\",\n",
        "                \"2) [선택적] 재스케일링 (단위 분산)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 9,\n",
        "            \"source_texts\": [\n",
        "                \"PCA Algorithm: Maximize Variance\",\n",
        "                \"Find unit vector u such that maximizes variance of projections\",\n",
        "                \"Note: $m \\approx m - 1$ for big data\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"PCA 알고리즘: 분산 극대화\",\n",
        "                \"투영의 분산을 극대화하는 단위 벡터 u를 찾는다\",\n",
        "                \"참고: 대용량 데이터의 경우 $m \\approx m - 1$\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 10,\n",
        "            \"source_texts\": [\n",
        "                \"Maximize Variance\",\n",
        "                \"In an optimization form\",\n",
        "                \"pick the largest eigenvalue $\\lambda_1$ of covariance matrix $S$\",\n",
        "                \"$u = u_1$ is the $\\lambda_1$'s corresponding eigenvector\",\n",
        "                \"$u_1$ is the first principal component (direction of highest variance in the data)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"분산 극대화\",\n",
        "                \"최적화 형태로 표현하면\",\n",
        "                \"공분산 행렬 $S$의 최대 고유값 $\\lambda_1$ 선택\",\n",
        "                \"여기서 $u = u_1$은 $\\lambda_1$에 해당하는 고유벡터\",\n",
        "                \"$u_1$은 첫 번째 주성분(데이터 내 최대 분산 방향)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 11,\n",
        "            \"source_texts\": [\n",
        "                \"Dimension Reduction Method ($n \\rightarrow k$)\",\n",
        "                \"1) Choose top $k$ (orthonormal) eigenvectors, $U = [u_1, u_2, ..., u_k]$\",\n",
        "                \"2) Project $x_i$ onto span {$u_1, u_2, ..., u_k$}\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"차원 축소 방법 ($n \\rightarrow k$)\",\n",
        "                \"1) 상위 $k$ (직교) 고유벡터 선택, $U = [u_1, u_2, ..., u_k]$\",\n",
        "                \"2) $x_i$를 {$u_1, u_2, ..., u_k$}의 스팬에 투영합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 12,\n",
        "            \"source_texts\": [\n",
        "                \"Dimensionality Reduction Using PCA\",\n",
        "                \" Calculate the covariance matrix of the data S\",\n",
        "                \" Calculate the eigen-vectors/eigen-values of S\",\n",
        "                \" Rank the eigen-values in decreasing order\",\n",
        "                \" Select eigen-vectors that retain a fixed percentage of the variance, \",\n",
        "                \"(e.g., 80%, the smallest $d$ such that (Sum i=1 to d of lambda_i) / (Sum of lambda_i) $\\geq 80\\%$)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"PCA를 이용한 차원 축소\",\n",
        "                \" 데이터의 공분산 행렬 S 계산\",\n",
        "                \"S의 고유 벡터/고유 값 계산\",\n",
        "                \"고유 값을 내림차순으로 정렬\",\n",
        "                \"분산의 일정 비율을 유지하는 고유벡터를 선택합니다.\",\n",
        "                \"(예: 80%, (Sum i=1 to d of lambda_i) / (Sum of lambda_i) $\\geq 80\\%$인 가장 작은 $d$)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 13,\n",
        "            \"source_texts\": [\n",
        "                \"PCA is a useful preprocessing step\",\n",
        "                \" Helps reduce computational complexity.\",\n",
        "                \" Can help supervised learning.\",\n",
        "                \" Reduced dimension ➔ simpler hypothesis space.\",\n",
        "                \" PCA can also be seen as noise reduction.\",\n",
        "                \" Caveats:\",\n",
        "                \" Fails when data consists of multiple separate clusters.\",\n",
        "                \" Directions of greatest variance may not be most informative (i.e., greatest classification power).\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"PCA는 유용한 전처리 단계입니다\",\n",
        "                \"계산 복잡도를 줄이는 데 도움이 됩니다.\",\n",
        "                \"지도 학습에 도움이 될 수 있습니다.\",\n",
        "                \"차원 축소 ➔ 더 단순한 가설 공간.\",\n",
        "                \"PCA는 잡음 감소로도 볼 수 있습니다.\",\n",
        "                \"주의 사항:\",\n",
        "                \"데이터가 여러 개의 분리된 클러스터로 구성될 때 실패합니다.\",\n",
        "                \"가장 큰 분산 방향이 가장 유용한 정보(즉, 가장 큰 분류 능력)를 제공하지 않을 수 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False],\n",
        "        }\n",
        "    ]\n",
        "},{\n",
        "    \"document_id\": \"NLP_10\",\n",
        "    \"pages\": [\n",
        "        {   \"page_num\": 2,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Input: a sequence of n tokens/words: \",\n",
        "                \"Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 \",\n",
        "                \"Output: a sequence of n labels, such that each token/word is associated with a label: \"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"입력: n개의 토큰/단어 시퀀스:\",\n",
        "                \"Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 \",\n",
        "                \"Output: a sequence of n labels, such that each token/word is associated with a label: \"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 3,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"POS Tagging\",\n",
        "                \"Assign POS tags to words\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"품사 태깅\",\n",
        "                \"단어에 품사 태그 할당\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 4,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Noun Phrase (NP) Chunking\",\n",
        "                \"Identify all non-recursive NP chunks\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"명사구(NP) 청킹\",\n",
        "                \"모든 비재귀적 NP 청크 식별\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 5,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"BIO Encoding\",\n",
        "                \" We define three new tags: \",\n",
        "                \" B-NP: beginning of a noun phrase chunk \",\n",
        "                \" I-NP: inside of a noun phrase chunk \",\n",
        "                \" O: outside of a noun phrase chunk\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"BIO 인코딩\",\n",
        "                \"우리는 세 가지 새로운 태그를 정의합니다:\",\n",
        "                \"B-NP: 명사구 청크의 시작\",\n",
        "                \"I-NP: 명사구 청크 내부\",\n",
        "                \"O: 명사구 청크 외부\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 6,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"OShallow Parsing\",\n",
        "                \" Identify all non-recursive NP, verb (“VP”) and preposition (“PP”) chunks\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"얕은 구문 분석\",\n",
        "                \"모든 비재귀적 명사구(NP), 동사구('VP'), 전치사구('PP')를 식별합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 7,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"BIO Encoding for Shallow Parsing\",\n",
        "                \" We define three new tags: \",\n",
        "                \" B-NP B-VP B-PP: beginning of an NP, “VP”, “PP” chunk \",\n",
        "                \" I-NP I-VP I-PP: inside of an NP, “VP”, “PP” chunk\",\n",
        "                \" O: outside of any chunk\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"얕은 구문 분석을 위한 BIO 인코딩\",\n",
        "                \" 세 가지 새로운 태그를 정의합니다:\",\n",
        "                \"B-NP B-VP B-PP: 명사구(NP), 동사구(VP), 전치사구(PP) 덩어리의 시작\",\n",
        "                \"I-NP I-VP I-PP: 명사구(NP), 동사구(VP), 전치사구(PP) 덩어리 내부\",\n",
        "                \"O: 모든 덩어리 외부\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, True, True, True],\n",
        "        },{\n",
        "            \"page_num\": 8,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Named Entity Recognition\",\n",
        "                \" Identify all mentions of named entities (people, organizations, locations, dates)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"명명된 엔티티 인식\",\n",
        "                \"명명된 엔티티(사람, 조직, 위치, 날짜)의 모든 언급 식별\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 9,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"BIO Encoding for NER\",\n",
        "                \" We define many new tags: \",\n",
        "                \" B-PERS, B-DATE,...: beginning of a mention of a person/ date... \",\n",
        "                \" I-PERS, I-DATE,...: inside of a mention of a person/date...\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"NER을 위한 BIO 인코딩\",\n",
        "                \"여러 새로운 태그를 정의합니다:\",\n",
        "                \"B-PERS, B-DATE,...: 인물/날짜 등의 언급 시작\",\n",
        "                \"I-PERS, I-DATE,...: 사람/날짜 등의 언급 내부\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, True, True],\n",
        "        },{\n",
        "            \"page_num\": 10,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"BIO Encoding for NER\",\n",
        "                \" BIO encoding can be used to frame any task that requires the identification of nonoverlapping and non-nested text spans as a sequence labeling problem, e.g.: \",\n",
        "                \" NP chunking \",\n",
        "                \" Shallow Parsing \",\n",
        "                \" Named entity recognition\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"NER을 위한 BIO 인코딩\",\n",
        "                \"BIO 인코딩은 중첩되지 않고 겹치지 않은 텍스트 범위의 식별이 필요한 모든 작업을 시퀀스 라벨링 문제로 구성하는 데 사용할 수 있습니다. 예:\",\n",
        "                \"명사구 분할\",\n",
        "                \"얕은 구문 분석\",\n",
        "                \"명명된 엔티티 인식\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 11,\n",
        "            \"source_texts\": [\n",
        "                \"Hidden Markov Models\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"숨겨진 마르코프 모델\"\n",
        "            ],\n",
        "            \"needs_correction\": [False],\n",
        "        },{\n",
        "            \"page_num\": 12,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"POS Tagging\",\n",
        "                \"She promised to back the bill\",\n",
        "                \"What is the most likely sequence of tags $t = t^{(1)}...t^{(N)}$ for the given sequence of words $w = w^{(1)}...w^{(N)}$?\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"품사 태깅\",\n",
        "                \"그녀는 법안을 지지하겠다고 약속했다\",\n",
        "                \"주어진 단어 시퀀스 $w = w^{(1)}...w^{(N)}$에 대해 가장 가능성이 높은 태그 시퀀스 $t = t^{(1)}...t^{(N)}$는 무엇인가?\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 13,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Hidden Markov Models (HMMs)\",\n",
        "                \"HMMs are the most commonly used generative models for POS tagging (and other tasks, e.g. in speech recognition)\",\n",
        "                \"HMMs make specific independence assumptions in P(t) and P(w|t):\",\n",
        "                \"P(t) is an n-gram (typically bigram or trigram) model over tags:\",\n",
        "                \"P(t_1^n) = ∏_i P(t_i | t_{i-1}, t_{i-2}) which are called transition probabilities\",\n",
        "                \"In p(W|t), each w_i depends only on [is generated by/conditioned on] t_i\",\n",
        "                \"P(W|t) = ∏_i P(w_i | t_i) which are called emission probabilities\",\n",
        "                \"These probabilities don’t depend on the position in the sentence (i), but are defined over word and tag types.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"숨겨진 마르코프 모델(HMM)\",\n",
        "                \"HMMs는 품사 태깅(및 음성 인식 등 다른 작업)에서 가장 일반적으로 사용되는 생성 모델입니다\",\n",
        "                \"HMMs는 P(t) 및 P(w|t)에서 특정 독립성 가정을 합니다:\",\n",
        "                \"P(t)는 태그에 대한 n-그램(일반적으로 바이그램 또는 트라이그램) 모델입니다:\",\n",
        "                \"P(t_1^n) = ∏_i P(t_i | t_{i-1}, t_{i-2})는 전이 확률이라고 불립니다\",\n",
        "                \"p(W|t)에서 각 w_i는 t_i에만 의존합니다 [t_i에 의해 생성되거나 조건부로 결정됩니다]\",\n",
        "                \"P(W|t) = ∏_i P(w_i | t_i)는 방출 확률이라고 불립니다\",\n",
        "                \"이러한 확률은 문장 내 위치(i)에 의존하지 않으며, 단어 및 태그 유형에 대해 정의됩니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 14,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Notation\",\n",
        "                \" To make the distinction between the i-th word/tag in the vocabulary/tag set and the i-th word/tag in the sentence clear: \",\n",
        "                \" Use superscript notation w^i for the i-th token in the sentence/sequence\",\n",
        "                \" and subscript notation w_i for the i-th type in the inventory (tagset/vocabulary)\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"기호\",\n",
        "                \"어휘/태그 세트의 i번째 단어/태그와 문장의 i번째 단어/태그를 명확히 구분하기 위해:\",\n",
        "                \"문장/시퀀스의 i번째 토큰에 대해 위 첨자 표기법 w^i를 사용하고\",\n",
        "                \"인벤토리(태그셋/어휘)의 i번째 유형에 대해 아래 첨자 표기법 w_i를 사용합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False, False],\n",
        "        },{\n",
        "            \"page_num\": 19,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Building a HMM tagger\",\n",
        "                \" To build a HMM tagger, we have to:\",\n",
        "                \"Train the model, i.e. estimate its parameters (the transition and emission probabilities) \",\n",
        "                \"Easy case: We have a corpus labeled with POS tags (supervised learning) \",\n",
        "                \"Harder case: We have a corpus, but it’s just raw text without tags (unsupervised learning). \",\n",
        "                \"In that case it really helps to have a dictionary of which POS tags each word can have \",\n",
        "                \"Define and implement a tagging algorithm that finds the best tag sequence t* for each input sentence W:\",\n",
        "                \"t* = argmax_T P(t)P(w|t)\",\n",
        "                \"Viterbi algorithm\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"HMM 태거 구축\",\n",
        "                \"HMM 태거를 구축하려면 다음을 수행해야 합니다:\",\n",
        "                \"모델을 훈련시키기, 즉 매개변수(전환 확률 및 발현 확률) 추정\",\n",
        "                \"쉬운 경우: 품사 태그가 지정된 코퍼스가 있는 경우 (지도 학습)\",\n",
        "                \"더 어려운 경우: 코퍼스가 있지만 태그가 없는 원본 텍스트만 있는 경우 (비지도 학습).\",\n",
        "                \"이 경우 각 단어가 가질 수 있는 품사 태그를 담은 사전이 있으면 정말 도움이 됩니다 \",\n",
        "                \"각 입력 문장 W에 대해 최상의 태그 시퀀스 t*를 찾는 태깅 알고리즘을 정의하고 구현하세요:\",\n",
        "                \"t* = argmax_T P(t)P(w|t)\",\n",
        "                \"비테르비 알고리즘\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 20,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Learning a HMM from labeled data\",\n",
        "                \" We count how often we see t_it_j and w_jt_i etc. in the data (use relative frequency estimates):\",\n",
        "                \" Learning the transition probabilities: P(t_j | t_i) = C(t_i, t_j) / C(t_i) \",\n",
        "                \" Learining the emission probabilities: P(w_j | t_i) = C(t_i, w_j) / C(t_i) \"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"레이블이 지정된 데이터에서 HMM 학습\",\n",
        "                \"데이터에서 t_it_j 및 w_jt_i 등의 출현 빈도를 계산합니다(상대 빈도 추정치 사용):\",\n",
        "                \"전이 확률 학습: P(t_j | t_i) = C(t_i, t_j) / C(t_i)\",\n",
        "                \"방출 확률 학습: P(w_j | t_i) = C(t_i, w_j) / C(t_i)\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 21,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"HMM decoding (Viterbi)\",\n",
        "                \"We are given a sentence w = w(1) ... w(N)\",\n",
        "                \"w =“she promised to back the bill”\",\n",
        "                \"We want to use an HMM tagger to find its POS tags \",\n",
        "                \"t* = argmax_T P(w,t)\",\n",
        "                \"But, with T tags, w has has O(T^N possible tag sequences!\",\n",
        "                \"To do this efficiently (in O(T^2N time), we will use a dynamic programming technique called the Viterbi algorithm which exploits the independence assumptions in the HMM.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"HMM 디코딩 (비테르비)\",\n",
        "                \"문장 w = w(1) ... w(N)\",\n",
        "                \"w = 'she promised to back the bill'\",\n",
        "                \"HMM 태거를 사용하여 해당 품사 태그를 찾고자 합니다\",\n",
        "                \"t* = argmax_T P(w,t)\",\n",
        "                \"하지만 T개의 태그가 있는 경우, w는 O(T^N)개의 가능한 태그 시퀀스를 가집니다!\",\n",
        "                \"이를 효율적으로 수행하기 위해 (O(T^2N) 시간 내에) 수행하기 위해, HMM의 독립성 가정을 활용하는 비테비 알고리즘이라는 동적 계획법 기법을 사용할 것입니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, True],\n",
        "        },{\n",
        "            \"page_num\": 22,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Dynamic Programming\",\n",
        "                \"Dynamic programming is a general technique to solve certain complex search problems by memoization\",\n",
        "                \"Recursively decompose the large search problem into smaller subproblems that can be solved efficiently\",\n",
        "                \"There is only a polynomial number of subproblems.\",\n",
        "                \"Store (memoize) the solutions of each subproblem in a common data structure\",\n",
        "                \"Processing this data structure takes polynomial time\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"동적 계획법\",\n",
        "                \"동적 계획법은 메모이제이션을 통해 특정 복잡한 탐색 문제를 해결하는 일반적인 기법입니다\",\n",
        "                \"대규모 탐색 문제를 효율적으로 해결할 수 있는 더 작은 하위 문제로 재귀적으로 분해합니다\",\n",
        "                \"하위 문제의 수는 다항식 수준에 불과합니다\",\n",
        "                \"각 하위 문제의 해법을 공통 데이터 구조에 저장(메모이제이션)합니다\",\n",
        "                \"이 데이터 구조를 처리하는 데는 다항식 시간이 소요됩니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 23,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"The Viterbi algorithm\",\n",
        "                \"A dynamic programming algorithm which finds the best (=most probable) tag sequence t* for an input sentence w: t* = argmax_T P(w|t)P(t)\",\n",
        "                \"Complexity: linear in the sentence length. With a bigram HMM, Viterbi runs in O(T^2N) steps for an input sentence with N words and a tag set of T tags.\",\n",
        "                \"The independence assumptions of the HMM tell us how to break up the big search problem (find t* = argmax_T P(w|t)P(t)) into smaller subproblems\",\n",
        "                \"The data structure used to store the solution of these subproblems is the trellis\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"비테비 알고리즘\",\n",
        "                \"동적 계획법 알고리즘으로, 입력 문장 w에 대해 최적(=가장 확률이 높은) 태그 시퀀스 t*를 찾는 알고리즘: t* = argmax_T P(w|t)P(t)\",\n",
        "                \"복잡도: 문장 길이에 선형적. 2그램 HMM을 사용하면, N개의 단어로 구성된 입력 문장과 T개의 태그 집합에 대해 Viterbi는 O(T^2N) 단계로 실행됩니다.\",\n",
        "                \"HMM의 독립성 가정은 거대한 검색 문제( t* = argmax_T P(w|t)P(t))를 더 작은 하위 문제로 분해하는 방법을 알려줍니다\",\n",
        "                \"이러한 하위 문제들의 해법을 저장하는 데 사용되는 데이터 구조는 트렐리스입니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, True, False, True, False, False],\n",
        "        },{\n",
        "            \"page_num\": 24,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"The Viterbi algorithm\",\n",
        "                \"We use a $N * T$ table (“trellis”) to keep track of the HMM. The HMM can assign one of the T tags to each of the N words.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"비테르비 알고리즘\",\n",
        "                \"우리는 HMM을 추적하기 위해 $N * T$ 테이블('트렐리스')을 사용합니다. HMM은 N개의 단어 각각에 T개의 태그 중 하나를 할당할 수 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 25,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"Viterbi: filling in the first column\",\n",
        "                \"We want to find the best (most likely) tag sequence for the entire sentence.\",\n",
        "                \"Each cell trellis[i][j] (corresponding to word W^(i) with tag t_j) contains:\",\n",
        "                \"trellis[i][j].viterbi: the probability of the best sequence ending in t_j\",\n",
        "                \"trellis[i][j].backpointer: to the cell k in the previous column that corresponds to the best tag sequence ending in t_j\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"비테르비: 첫 번째 열 채우기\",\n",
        "                \"우리는 전체 문장에 대한 최상의(가장 가능성이 높은) 태그 시퀀스를 찾고자 합니다.\",\n",
        "                \"각 셀 trellis[i][j] (단어 W^ (i)에 태그 t_j가 할당됨)에는 다음이 포함됩니다:\",\n",
        "                \"trellis[i][j].viterbi: t_j에서 끝나는 최적 시퀀스의 확률\",\n",
        "                \"trellis[i][j].backpointer: t_j에서 끝나는 최적 태그 시퀀스에 해당하는 이전 열의 셀 k로 연결됨\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 27,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"At any internal cell\",\n",
        "                \"For each cell in the preceding column: multiply its Viterbi probability with the transition probability to the current cell. \",\n",
        "                \"Keep a single backpointer to the best (highest scoring) cell in the preceding column \",\n",
        "                \"Multiply this score with the emission probability of the current word\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"내부 셀에서\",\n",
        "                \"이전 열의 각 셀에 대해: 해당 셀의 비테르비 확률에 현재 셀로의 전이 확률을 곱합니다.\",\n",
        "                \"이전 열에서 가장 높은 점수를 받은 최상의 셀로 향하는 단일 백포인터를 유지합니다\",\n",
        "                \"이 점수를 현재 단어의 방출 확률과 곱합니다\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 28,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"At the end of the sentence\",\n",
        "                \"In the last column (i.e. at the end of the sentence) pick the cell with the highest entry and trace back the backpointers to the first word in the sentence.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"문장 끝에서\",\n",
        "                \"마지막 열(즉, 문장 끝)에서 가장 높은 진입 확률을 가진 셀을 선택하고 백포인터를 따라 문장의 첫 단어로 추적합니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 29,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling\",\n",
        "                \"At the end of the sentence\",\n",
        "                \"By keeping one backpointer from each cell to the cell in the previous column that yields the highest probability, we can retrieve the most likely tag sequence when we’re done.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링\",\n",
        "                \"문장 끝에서\",\n",
        "                \"각 셀에서 이전 열의 셀 중 가장 높은 확률을 산출하는 셀로 향하는 백포인터 하나를 유지함으로써, 작업을 완료했을 때 가장 가능성이 높은 태그 시퀀스를 복구할 수 있습니다.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 46,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling Algorithms\",\n",
        "                \"Sequence Labeling Models\",\n",
        "                \"Statistical models: \",\n",
        "                \"Maximum Entropy Markov Models (MEMMs) \",\n",
        "                \"Conditional Random Fields (CRFs) \",\n",
        "                \"Neural models: \",\n",
        "                \"Recurrent networks (or transformers) that predict a label at each time step, possibly with a CRF output layer.\"\n",
        "            ],\n",
        "            \"machine_translation\": [\n",
        "                \"시퀀스 라벨링 알고리즘\",\n",
        "                \"시퀀스 라벨링 모델\",\n",
        "                \"통계적 모델:\",\n",
        "                \"최대 엔트로피 마르코프 모델(MEMMs)\",\n",
        "                \"조건부 랜덤 필드(CRFs)\",\n",
        "                \"신경망 모델:\",\n",
        "                \"각 시간 단계에서 레이블을 예측하는 재귀 신경망(또는 트랜스포머), 경우에 따라 CRF 출력 레이어를 포함할 수 있음.\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False],\n",
        "        },{\n",
        "            \"page_num\": 49,\n",
        "            \"source_texts\": [\n",
        "                \"Sequence Labeling Algorithms\",\n",
        "                \"Conditional Random Fields (CRFs)\",\n",
        "                \"Conditional Random Fields have the same mathematical definition as MEMMs, but: \",\n",
        "                \"CRFs are trained globally to maximize the probability of the overall sequence,\",\n",
        "                \"MEMMs are trained locally to maximize the probability of each individual label \",\n",
        "                \"This requires dynamic programming \",\n",
        "                \"Training: akin to the Forward-Backward algorithm used to train HMMs from unlabeled sequences\",\n",
        "                \"Decoding: Viterbi\"\n",
        "            ]\n",
        "            ,\"machine_translation\": [\n",
        "                \"시퀀스 라벨링 알고리즘\",\n",
        "                \"조건부 랜덤 필드(CRFs)\",\n",
        "                \"조건부 랜덤 필드는 MEMMs와 동일한 수학적 정의를 가지지만:\",\n",
        "                \"CRFs는 전체 시퀀스의 확률을 극대화하기 위해 전역적으로 훈련됩니다.\",\n",
        "                \"MEMMs는 각 개별 레이블의 확률을 극대화하기 위해 국소적으로 훈련됩니다\",\n",
        "                \"이는 동적 프로그래밍을 필요로 합니다\",\n",
        "                \"훈련:  라벨이 없는 시퀀스로부터 HMM을 훈련하는 데 사용되는 전진-후진 알고리즘과 유사합니다.\",\n",
        "                \"디코딩: 비테르비\"\n",
        "            ],\n",
        "            \"needs_correction\": [False, False, False, False, False, False, False],\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "]\n",
        "\n",
        "def flatten_data_for_csv_conversion(data_list):\n",
        "    rows = []\n",
        "\n",
        "    for doc_data in data_list:\n",
        "        doc_id = doc_data.get(\"document_id\", \"Unknown_Doc\")\n",
        "        for page in doc_data.get(\"pages\", []):\n",
        "            page_num = page.get(\"page_num\", 0)\n",
        "            source_texts = page.get(\"source_texts\", [])\n",
        "            # 'needs_correction'과 'machine_translation' 키가 없으면 빈 리스트로 처리\n",
        "            mt = page.get(\"machine_translation\", [''] * len(source_texts))\n",
        "            nc = page.get(\"needs_correction\", [None] * len(source_texts))\n",
        "\n",
        "            # source_texts, machine_translation, needs_correction 리스트를 동시에 순회\n",
        "            for text, translated, correction_needed in zip(source_texts, mt, nc):\n",
        "                if text and text.strip():\n",
        "                    # CSV 행으로 데이터 추가\n",
        "                    rows.append({\n",
        "                        \"document_id\": doc_id,\n",
        "                        \"page_num\": page_num,\n",
        "                        \"source_text\": text.strip(),\n",
        "                        \"machine_translation\": translated.strip() if translated else \"\",\n",
        "                        \"needs_correction\": correction_needed\n",
        "                    })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_final = flatten_data_for_csv_conversion(data_list)\n",
        "FINAL_CSV_NAME = \"강의자료_EDA_최종번역완료.csv\"\n",
        "\n",
        "# to_csv를 사용하여 CSV 파일 생성 (쉼표 충돌 자동 처리)\n",
        "df_final.to_csv(FINAL_CSV_NAME, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\" CSV 파일 재변환 완료: {FINAL_CSV_NAME}\")\n",
        "print(f\"총 {len(df_final)}개 문장 최종 로드 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL5yNn0zG_wq",
        "outputId": "06a5cad1-c33d-4618-a9d2-d890f74b00cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CSV 파일 재변환 완료: 강의자료_EDA_최종번역완료.csv\n",
            "총 476개 문장 최종 로드 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:632: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:633: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:637: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:638: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:645: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:649: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:856: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:857: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:862: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:863: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:870: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:875: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:896: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:899: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:903: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:906: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:1266: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1267: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1273: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1274: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1299: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:1307: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:632: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:633: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:637: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:638: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:645: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:649: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:856: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:857: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:862: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:863: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:870: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:875: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:896: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:899: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:903: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:906: SyntaxWarning: invalid escape sequence '\\h'\n",
            "<>:1266: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1267: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1273: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1274: SyntaxWarning: invalid escape sequence '\\l'\n",
            "<>:1299: SyntaxWarning: invalid escape sequence '\\g'\n",
            "<>:1307: SyntaxWarning: invalid escape sequence '\\g'\n",
            "/tmp/ipython-input-3853349212.py:632: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  \"Compute the loss $\\mathcal{L}_t$ by comparing $\\hat{y}_t$ and $y_t$ ($y_t$ is ground truth)\",\n",
            "/tmp/ipython-input-3853349212.py:633: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  \"e.g., $\\mathcal{L}_t = (\\hat{y}_t - y_t)^2$\"\n",
            "/tmp/ipython-input-3853349212.py:637: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  \"$\\hat{y}_t$와 $y_t$($y_t$는 실제 값)를 비교하여 손실 $\\mathcal{L}_t$를 계산합니다\",\n",
            "/tmp/ipython-input-3853349212.py:638: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  \"예: $\\mathcal{L}_t = (\\hat{y}_t - y_t)^2$\"\n",
            "/tmp/ipython-input-3853349212.py:645: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  \"Total loss $\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t$\"\n",
            "/tmp/ipython-input-3853349212.py:649: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  \"전체 손실 $\\mathcal{L} = \\sum_{t=1}^T \\mathcal{L}_t$\"\n",
            "/tmp/ipython-input-3853349212.py:856: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"When $\\sigma$ outputs 0, the network will “completely forget” the information from $c_{t-1}$.\",\n",
            "/tmp/ipython-input-3853349212.py:857: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"When $\\sigma$ outputs 1, “completely keep”\"\n",
            "/tmp/ipython-input-3853349212.py:862: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$가 0을 출력하면 네트워크는 $c_{t-1}$의 정보를 '완전히 잊어버린다.'\",\n",
            "/tmp/ipython-input-3853349212.py:863: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$가 1을 출력하면 '완전히 유지'합니다.\"\n",
            "/tmp/ipython-input-3853349212.py:870: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$ decides what values to update\",\n",
            "/tmp/ipython-input-3853349212.py:875: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$는 업데이트할 값을 결정합니다\",\n",
            "/tmp/ipython-input-3853349212.py:896: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$ decides what parts of the cell state to output as current hidden state\",\n",
            "/tmp/ipython-input-3853349212.py:899: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  \"$h_t$ will be used to compute $\\hat{y}_t$\"\n",
            "/tmp/ipython-input-3853349212.py:903: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"$\\sigma$는 셀 상태의 어떤 부분을 현재 숨겨진 상태로 출력할지 결정합니다\",\n",
            "/tmp/ipython-input-3853349212.py:906: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  \"$h_t$는 $\\hat{y}_t$를 계산하는 데 사용될 것입니다\"\n",
            "/tmp/ipython-input-3853349212.py:1266: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  \"pick the largest eigenvalue $\\lambda_1$ of covariance matrix $S$\",\n",
            "/tmp/ipython-input-3853349212.py:1267: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  \"$u = u_1$ is the $\\lambda_1$'s corresponding eigenvector\",\n",
            "/tmp/ipython-input-3853349212.py:1273: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  \"공분산 행렬 $S$의 최대 고유값 $\\lambda_1$ 선택\",\n",
            "/tmp/ipython-input-3853349212.py:1274: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  \"여기서 $u = u_1$은 $\\lambda_1$에 해당하는 고유벡터\",\n",
            "/tmp/ipython-input-3853349212.py:1299: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  \"(e.g., 80%, the smallest $d$ such that (Sum i=1 to d of lambda_i) / (Sum of lambda_i) $\\geq 80\\%$)\"\n",
            "/tmp/ipython-input-3853349212.py:1307: SyntaxWarning: invalid escape sequence '\\g'\n",
            "  \"(예: 80%, (Sum i=1 to d of lambda_i) / (Sum of lambda_i) $\\geq 80\\%$인 가장 작은 $d$)\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 한글 폰트 설치\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.matplotlib # 폰트 캐시 삭제\n",
        "\n",
        "# 2. Matplotlib 런타임 환경 설정\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 폰트 경로 설정\n",
        "fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "\n",
        "# Matplotlib 설정 변경\n",
        "plt.rcParams['font.family'] = 'NanumGothic'  # NanumGothic으로 폰트 설정\n",
        "plt.rcParams['axes.unicode_minus'] = False   # 마이너스 기호 깨짐 방지"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4JAgmBSFe5o",
        "outputId": "a7c5b73a-78a6-4ae5-9731-656915f9f026"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20200506-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "CSV_FILE_NAME = \"강의자료_EDA_최종번역완료.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(CSV_FILE_NAME)\n",
        "    df['needs_correction'] = df['needs_correction'].replace({'True': True, 'False': False, True: True, False: False}).fillna(False).astype(bool)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"파일을 찾을 수 없습니다. \")\n",
        "    exit()\n",
        "\n",
        "## 데이터 크기, 컬럼 및 결측값 확인\n",
        "print(\"\\n--- 1. 데이터 크기 및 컬럼 정보 (df.info()) ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n--- 2. 결측값(NaN) 확인 ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- 3. 데이터 상위 5개 행 (df.head()) ---\")\n",
        "print(df.head().to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frrCKFdEE2lK",
        "outputId": "6441e6fd-f8c0-4a4f-a6da-4794e9635e1a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. 데이터 크기 및 컬럼 정보 (df.info()) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 476 entries, 0 to 475\n",
            "Data columns (total 5 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   document_id          476 non-null    object\n",
            " 1   page_num             476 non-null    int64 \n",
            " 2   source_text          476 non-null    object\n",
            " 3   machine_translation  476 non-null    object\n",
            " 4   needs_correction     476 non-null    bool  \n",
            "dtypes: bool(1), int64(1), object(3)\n",
            "memory usage: 15.5+ KB\n",
            "\n",
            "--- 2. 결측값(NaN) 확인 ---\n",
            "document_id            0\n",
            "page_num               0\n",
            "source_text            0\n",
            "machine_translation    0\n",
            "needs_correction       0\n",
            "dtype: int64\n",
            "\n",
            "--- 3. 데이터 상위 5개 행 (df.head()) ---\n",
            "| document_id         |   page_num | source_text                                    | machine_translation                             | needs_correction   |\n",
            "|:--------------------|-----------:|:-----------------------------------------------|:------------------------------------------------|:-------------------|\n",
            "| ml4e-lecture-week10 |          3 | Deep Learning                                  | 딥 러닝                                         | False              |\n",
            "| ml4e-lecture-week10 |          3 | Convolutional Neural Networks                  | 컨볼루션 신경망                                 | True               |\n",
            "| ml4e-lecture-week10 |          4 | What Computers “See”?                          | 컴퓨터가 '보는' 것은 무엇인가?                  | False              |\n",
            "| ml4e-lecture-week10 |          4 | Images are numbers                             | 이미지는 숫자이다                               | False              |\n",
            "| ml4e-lecture-week10 |          4 | An image is just a matrix of numbers [0, 255]! | 이미지는 단지 [0, 255]범위의 숫자의 행렬입니다! | False              |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. source_text의 길이 (단어 수) 계산\n",
        "# 공백을 기준으로 단어 수 계산\n",
        "df['source_length'] = df['source_text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "print(\"\\n--- 1. 문장 길이 (단어 수) 기술 통계 ---\")\n",
        "print(df['source_length'].describe().to_markdown())\n",
        "\n",
        "# 2. needs_correction (오류율) 기본 통계\n",
        "error_rate = df['needs_correction'].mean() * 100\n",
        "print(f\"\\n전체 문장 중 수정이 필요한 문장(오류율): {error_rate:.2f}%\")\n",
        "\n",
        "# 3. 오류 문장과 정상 문장의 평균 길이 비교\n",
        "avg_len_error = df[df['needs_correction'] == True]['source_length'].mean()\n",
        "avg_len_clean = df[df['needs_correction'] == False]['source_length'].mean()\n",
        "\n",
        "print(f\"수정 필요한 문장의 평균 길이: {avg_len_error:.2f} 단어\")\n",
        "print(f\"수정 불필요한 문장의 평균 길이: {avg_len_clean:.2f} 단어\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvG-eYM0E96Y",
        "outputId": "8c784f43-7a91-4f7c-ef17-f376616704d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. 문장 길이 (단어 수) 기술 통계 ---\n",
            "|       |   source_length |\n",
            "|:------|----------------:|\n",
            "| count |       476       |\n",
            "| mean  |         8.18067 |\n",
            "| std   |         6.05074 |\n",
            "| min   |         1       |\n",
            "| 25%   |         4       |\n",
            "| 50%   |         6       |\n",
            "| 75%   |        12       |\n",
            "| max   |        44       |\n",
            "\n",
            "전체 문장 중 수정이 필요한 문장(오류율): 6.72%\n",
            "수정 필요한 문장의 평균 길이: 8.31 단어\n",
            "수정 불필요한 문장의 평균 길이: 8.17 단어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화 설정\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "\n",
        "# 1. 문장 길이 분포 (히스토그램)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['source_length'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('문장 길이 분포 (단어 수)')\n",
        "plt.xlabel('문장 길이 (단어 수)')\n",
        "plt.ylabel('빈도')\n",
        "plt.show()\n",
        "\n",
        "# 2. 문장 길이와 오류율의 관계 (박스 플롯)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='needs_correction', y='source_length', data=df, palette=['lightgreen', 'salmon'])\n",
        "plt.title('문장 길이와 수정 필요 여부(오류)의 관계')\n",
        "plt.xlabel('수정 필요 여부 (False: 수정 불필요, True: 수정 필요)')\n",
        "plt.ylabel('문장 길이 (단어 수)')\n",
        "plt.xticks([0, 1], ['수정 불필요 (False)', '수정 필요 (True)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JDlRDxQ_E5Lx",
        "outputId": "aa6ab342-5b64-46ab-a1e9-e444585e3400"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-276382287.py:15: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.boxplot(x='needs_correction', y='source_length', data=df, palette=['lightgreen', 'salmon'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
