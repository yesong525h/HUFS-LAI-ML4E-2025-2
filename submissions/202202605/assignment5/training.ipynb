{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnl4I1C13MuOnZUqytWJdX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yesong525h/HUFS-LAI-ML4E-2025-2/blob/patch-2/submissions/202202605/assignment5/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë“  .ipynb íŒŒì¼ì˜ ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "_-0tT7O8il6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training.ipynb\n",
        "\n",
        "# =========================================================\n",
        "# ì…€ 1: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì´ˆê¸° ì„¤ì •\n",
        "# =========================================================\n",
        "!pip install transformers datasets accelerate pandas scikit-learn torch -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Assignment5_Files/my_word_classifier\"\n",
        "MODEL_NAME = \"klue/bert-base\"\n",
        "MAX_LENGTH = 32\n"
      ],
      "metadata": {
        "id": "9WDjOHT_c81X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì…€ 2: ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
        "# (ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ì—­í• )\n",
        "# =========================================================\n",
        "class WordDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word = str(self.data.iloc[idx]['word'])\n",
        "        label = int(self.data.iloc[idx]['label'])\n",
        "\n",
        "        # ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (í† í°í™”)\n",
        "        inputs = self.tokenizer(\n",
        "            word,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "I9vmWR6GdvVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì…€ 3: ë°ì´í„° ë¡œë“œ ë° Train/Val/Test ë¶„í• \n",
        "# (Code 1ì˜ ë‚´ìš©)\n",
        "# =========================================================\n",
        "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv(\"word_labels_final.csv\")\n",
        "\n",
        "# 2. ë°ì´í„° ì²­ì†Œ (ë¼ë²¨ì´ ë¹„ì–´ìˆëŠ” í–‰ ì‚­ì œ ë° ì •ìˆ˜ ë³€í™˜)\n",
        "df = df.dropna(subset=['label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# 3. ë°ì´í„° ë‚˜ëˆ„ê¸° (Train 80% : Validation 10% : Test 10%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "# training.ipynbì˜ ë°ì´í„° ì €ì¥ ì½”ë“œ ì§ì „ì— ì•„ë˜ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\n",
        "\n",
        "import os # ë§¨ ìœ„ì— ì´ë¯¸ import ë˜ì–´ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "# ... (ì´ì „ ì½”ë“œ ìƒëµ)\n",
        "\n",
        "# 4. í‰ê°€ìš©(Test) ë°ì´í„° ì €ì¥ ìœ„ì¹˜ë¥¼ Driveë¡œ ë³€ê²½\n",
        "TEST_CSV_PATH = \"/content/drive/MyDrive/Assignment5_Files/test.csv\"\n",
        "\n",
        "# **[â­ï¸ ì—¬ê¸°ë¥¼ ìˆ˜ì •/ì¶”ê°€í•˜ì„¸ìš” â­ï¸]**\n",
        "# íŒŒì¼ ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ë§Œ ì¶”ì¶œ\n",
        "dir_path = os.path.dirname(TEST_CSV_PATH)\n",
        "# ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (exist_ok=TrueëŠ” ì´ë¯¸ í´ë”ê°€ ìˆì–´ë„ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ)\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "# **[â­ï¸ ì—¬ê¸°ê¹Œì§€ ì¶”ê°€ â­ï¸]**\n",
        "\n",
        "test_df.to_csv(TEST_CSV_PATH, index=False)\n",
        "print(f\"ğŸ‘‰ Test Setì´ '{TEST_CSV_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "C_ObSICad2FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì…€ 4: ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
        "# (Code 2ì˜ ë‚´ìš©)\n",
        "# =========================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "train_dataset = WordDataset(train_df, tokenizer)\n",
        "val_dataset = WordDataset(val_df, tokenizer)\n",
        "\n",
        "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° GPU ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {device}\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "9ltfh8fSd4o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì…€ 5: í•™ìŠµ ì„¤ì • ì •ì˜ ë° í•™ìŠµ ì‹œì‘\n",
        "# (Code 3ì˜ ë‚´ìš©)\n",
        "# =========================================================\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # ì™¸ë¶€ ë¡œê¹… íˆ´ ë„ê¸°\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=2e-5,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "print(\"5. ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì•½ 5 epoch)...\")\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "rvLPWRICd66V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ì…€ 6: ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "# =========================================================\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Assignment5_Files/my_word_classifier\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "print(f\"\\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ '{SAVE_PATH}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "3bPLmiRZd8vJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}